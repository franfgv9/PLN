{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franfgv9/PLN/blob/main/Practica1_PLN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Arthur Smith bought a Samsung phone in Macau . It was cheaperthan in his country . \"\"\""
      ],
      "metadata": {
        "id": "wjPbt6kPHbnP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK"
      ],
      "metadata": {
        "id": "wYGGrvOMFlxQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyWhyt1uE6XF",
        "outputId": "39498c59-c5d0-45ed-e4f0-e44f0241bb95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "# data required for the examples and exercises in the book\n",
        "nltk . download ( \"book\")\n",
        "nltk . download ('punkt_tab')\n",
        "nltk . download ('averaged_perceptron_tagger_eng')\n",
        "nltk . download ('maxent_ne_chunker_tab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy\n"
      ],
      "metadata": {
        "id": "IUMH2vswFpt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "O2ZqUDB3FuLW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJEMPLO 1.1"
      ],
      "metadata": {
        "id": "e9PJsEqiGTc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizar un texto en frases\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1uuQ66PGVC6",
        "outputId": "d5a33228-dc44-4930-ecb5-6a572b254fca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Arthur Smith bought a Samsung phone in Macau .', 'It was cheaperthan in his country .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizar cada frase del texto en palabras\n",
        "\n",
        "for sent in sentences:\n",
        "  tokens = nltk.word_tokenize(sent)\n",
        "  print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3Vh2oLvGmDh",
        "outputId": "5a7c9474-4788-4993-c4ce-5f59137f0862"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Arthur', 'Smith', 'bought', 'a', 'Samsung', 'phone', 'in', 'Macau', '.']\n",
            "['It', 'was', 'cheaperthan', 'in', 'his', 'country', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Te da los verbos y su POS tagset (si es un verbo, adjetivo, etc)\n",
        "\n",
        "tagged = nltk . pos_tag ( tokens )\n",
        "print ( tagged )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwNpk_OFfXrh",
        "outputId": "fc5897ff-8922-4fd7-e902-e022b596bbc1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('It', 'PRP'), ('was', 'VBD'), ('cheaperthan', 'VBN'), ('in', 'IN'), ('his', 'PRP$'), ('country', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entities = nltk . chunk . ne_chunk ( tagged )\n",
        "print ( entities )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeLhwHyDfbqE",
        "outputId": "2c4e12dc-e578-48bd-844d-5b1498e9a7af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S It/PRP was/VBD cheaperthan/VBN in/IN his/PRP$ country/NN ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercício 1.2"
      ],
      "metadata": {
        "id": "gBU6-fDfK2Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk . corpus import floresta\n",
        "nltk . download ('floresta')"
      ],
      "metadata": {
        "id": "wBvLbG4RK26D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2521b96-6141-440c-dd89-126709470ae7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/floresta.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de palavras\n",
        "words = floresta . words ()\n",
        "# Lista de frases tokenizadas\n",
        "sentences = floresta . sents ()"
      ],
      "metadata": {
        "id": "u_HK05nOhpGB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para imprimir el numero de palabras (tokens) y el numero de los distintos tipos de palabras\n",
        "\n",
        "fd = nltk . FreqDist ( words )\n",
        "print ('Tokens', len ( words ) )\n",
        "print ('Types', len ( fd ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_bhy2bLhwxI",
        "outputId": "024b03ab-d1fa-4922-a1f3-b6214944223f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens 211852\n",
            "Types 29421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para imprimir las palabras más frecuentes\n",
        "\n",
        "print ('Most frequent word ', fd . max () )\n",
        "print ('10 most frequent words ', fd . most_common (10) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3veKumxiBi9",
        "outputId": "770e6ddc-17c6-4902-c57a-2478e36cc288"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent word  de\n",
            "10 most frequent words  [('de', 14569), (',', 13444), ('a', 12656), ('o', 10025), ('.', 7725), ('em', 5505), ('e', 3981), ('que', 3956), ('os', 3223), ('«', 2369)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJERCICIO 1"
      ],
      "metadata": {
        "id": "JYzIRxz1iYnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopwords --> palabras irrelevantes (el, a, que, etc)\n",
        "import string\n",
        "\n",
        "# Carga la lista de stopwords en portugués usando NLTK.\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "# print(stopwords[:10])\n",
        "\n",
        "fd = nltk.FreqDist(w.lower() for w in floresta.words()\n",
        "                   if w.lower() not in stopwords and w not in string.punctuation)\n",
        "most_frequent = fd.most_common(20)\n",
        "for w in most_frequent:\n",
        "    print(w)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6CYncctiTfv",
        "outputId": "d687b671-4f23-4be7-872d-4cf63e485354"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJERCICIO 1.2"
      ],
      "metadata": {
        "id": "W1Wa1Jt0l2GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧠 Objetivo del código\n",
        "  # El propósito es reducir las palabras a su raíz (stem) antes de calcular su frecuencia.\n",
        "  # Esto permite agrupar palabras que tienen el mismo significado base, aunque estén en distintas formas (género, número, conjugación…)."
      ],
      "metadata": {
        "id": "edtarZTMmv9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Un stemmer corta las palabras hasta su raíz o base común.\n",
        "nltk . download ('rslp')\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "print ( stemmer . stem ('grande') )\n",
        "print ( stemmer . stem ('grandes') )\n",
        "print ( stemmer . stem ('grandinha') )\n",
        "print ( stemmer . stem ('grandalhão') )\n",
        "print ( stemmer . stem ('ser') )\n",
        "print ( stemmer . stem ('sou') )\n",
        "print ( stemmer . stem ('fui') )\n",
        "\n",
        "print('FLORESTA ####')\n",
        "fd = nltk.FreqDist(stemmer.stem(w.lower())\n",
        "                   for w in floresta.words()\n",
        "                   if w.lower() not in stopwords and w not in string.punctuation)\n",
        "\n",
        "most_frequent = fd.most_common(20)\n",
        "for mf in most_frequent:\n",
        "    print(mf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0tr4HL6l4TF",
        "outputId": "e90ea835-d701-445d-9124-444f5185e3ea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grand\n",
            "grand\n",
            "grand\n",
            "grand\n",
            "ser\n",
            "sou\n",
            "fui\n",
            "FLORESTA ####\n",
            "('«', 2369)\n",
            "('»', 2310)\n",
            "('ano', 491)\n",
            "('pod', 445)\n",
            "('pass', 361)\n",
            "('outr', 354)\n",
            "('sobr', 354)\n",
            "('segund', 319)\n",
            "('nov', 319)\n",
            "('prim', 300)\n",
            "('dia', 299)\n",
            "('ont', 292)\n",
            "('aind', 279)\n",
            "('part', 278)\n",
            "('ter', 270)\n",
            "('cas', 262)\n",
            "('dev', 260)\n",
            "('govern', 254)\n",
            "('faz', 248)\n",
            "('jog', 247)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJEMPLO 1.4"
      ],
      "metadata": {
        "id": "tyJC52W4nmB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta función busca una palabra dentro del corpus y muestra cómo aparece en su contexto, es decir, qué palabras tiene alrededor."
      ],
      "metadata": {
        "id": "_-n4FgGVpHeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concordance ( corpus , word , context =30) :\n",
        "  for sent in corpus . sents () :\n",
        "    if word in sent :\n",
        "      pos = sent . index ( word )  # Obtiene la posición (índice) de la palabra dentro de la oración\n",
        "      left = ' '. join ( sent [: pos ])   # : todas las palabras que hay antes de la palabra buscada\n",
        "      right = ' '. join ( sent [ pos +1:])\n",
        "      print ('%*s %s %-*s' % ( context , left [ - context :] , word , context , right [:context ]) )"
      ],
      "metadata": {
        "id": "DiMW2UFknn2h"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concordance(floresta,'grande', context=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XZDVwqnn6Ja",
        "outputId": "e9d8edfb-bdc3-4402-9843-dfaf3351e62b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " de a equipa não ajudasse grande coisa .                  \n",
            " igualdade através de uma grande penalidade marcada por Ru\n",
            " os últimos 24 meses em a grande Casa_Comum_Europeia .    \n",
            " durante toda_a noite , o grande veleiro foi tomado de ass\n",
            "e ele acaba por ir- se em grande dificuldade .            \n",
            "carros estacionados , uma grande quantidade de pequenos le\n",
            "Guimarães está a suscitar grande interesse em a região min\n",
            "tos de o país » exigem um grande controlo de os gastos .  \n",
            "           Mas o primeiro grande atleta de Moniz_Pereira v\n",
            "ís que têm uma atitude de grande dignidade por os animais \n",
            "mpos bateu patinadoras de grande nome como a alemã Tanja_S\n",
            "                      E a grande dúvida é se as suspeitas \n",
            "menos em_relação_a aquele grande estadista , que penso ser\n",
            "riedade de os edifícios , grande parte de eles foram ocupa\n",
            "        Percebe- se que a grande massa de gente está ali m\n",
            "inas de filmar forçaram o grande êxodo .                  \n",
            "até a o fim de a primeira grande guerra dirigiu e supervis\n",
            "rte portuguesa de a nossa grande equipa tem sido eficaz e \n",
            " que podem significar uma grande revolução para o mundo de\n",
            "                        A grande questão é saber se havia \n",
            "ucação , que vê em ele um grande futuro .                 \n",
            "obilização , pois « o seu grande intuito foi despolitizar \n",
            " A União_Nacional era uma grande frente onde Salazar toler\n",
            "qualquer , de_preferência grande .                        \n",
            ", onde McStay se exibia a grande altura .                 \n",
            "legação nenhuma , uma tão grande afluência de sócios para \n",
            "ter-Fiorentina era o jogo grande de a terceira jornada de \n",
            "               Uma medida grande para o seu corpo pesado ,\n",
            "      Um aviso fugaz em o grande mercado de as aves , quan\n",
            " , terá dois encontros de grande emotividade ;            \n",
            "fe_do_Estoril continua em grande ;                        \n",
            "  Motor ; Económico , com grande elasticidade e bom nível \n",
            "es de Maastricht « são de grande importância para o país »\n",
            "s de rotina a os museus , grande parte de os washingtonian\n",
            " Lisboa , 1994 } leve uma grande volta .                  \n",
            "                        O grande êxito de o Vaticano em a \n",
            "                    Mas o grande motivo de o atraso em o a\n",
            "  Esta é aliás a primeira grande surpresa de 1994 , que a \n",
            "nto de a morte de o seu « grande amigo » , evocou um encon\n",
            "scender a o paraíso , tão grande é a beleza celestial de a\n",
            " um grau de incerteza tão grande que , a o ser accionada ,\n",
            "nciou o pré-programa de a grande celebração de os estudant\n",
            "assado , apresentámos com grande sucesso vários conjuntos \n",
            "Desde_logo por_causa_de o grande afluxo de público        \n",
            "r , para essa geração , a grande escola iniciática para qu\n",
            " Alberto_João , não era « grande espingarda » e « antipáti\n",
            " o ministro já manifestou grande curiosidade quanto_a o qu\n",
            "correndo o seu flanco com grande à-vontade .              \n",
            "fastadas ; arquitectura e grande público .                \n",
            "                        A grande dúvida que circula entre \n",
            "a- se- uma recuperação de grande categoria , em a_qual alc\n",
            "s anos 40 » renovando « a grande tradição de Hammet e Chan\n",
            " o referendo dentro_de um grande envelope .               \n",
            "mia familiar são ainda em grande número , segundo informou\n",
            " é considerada o primeiro grande instrumento de trabalho e\n",
            "o Vaticano ou provoca uma grande polémica e torna- se um s\n",
            "e os 85 ienes e mostrando grande firmeza .                \n",
            "ou um pneu e ouviu- se um grande estrondo .               \n",
            "                     Jogo grande foi o que juntou , em Pho\n",
            "ara em alguns momentos de grande polémica .               \n",
            "mos no_caso_de África , a grande quantidade de gente que a\n",
            " notou que « era a irmã o grande apoio em os estudos » , c\n",
            "dependente de uma obra de grande envergadura , ainda sem l\n",
            "bem , não parecem ter uma grande cultura própria .        \n",
            "recurso a uma mira , é de grande utilidade .              \n",
            "   De esse dia conserva o grande recorte de jornal e uma c\n",
            "como o diz , terá sido um grande alívio .                 \n",
            "              Ainda_que a grande preocupação de o público \n",
            "jectos de expansão { para grande dor-de-cabeça de as marca\n",
            "dades nacionais não viram grande receptividade e decidiram\n",
            " e que reenvia para a sua grande escultura de ninfas com u\n",
            " de poupadores de médio e grande porte em_tese , menos suj\n",
            "         « O Helmet é uma grande seção rítmica » , definiu\n",
            "ho de sete cabeças » para grande parte de os estudantes de\n",
            "                     É um grande pintor , e sua última fas\n",
            "o são ramificações de uma grande organização criminosa .  \n",
            "gres » seria recebido sem grande estardalhaço .           \n",
            "                 Essa é a grande novidade de o Carnaval_94\n",
            "     A agitação ontem foi grande também em a Bolsa_de_Merc\n",
            " sinto uma angústia muito grande » , afirmou .            \n",
            "ntes de as torcidas de os grande times de São_Paulo { Cori\n",
            "o vou saber se meu amor é grande o suficiente ? »         \n",
            "sta , porque o amor não é grande nem pequeno .            \n",
            "                        A grande vantagem de os fundos de \n",
            "a parte mais crucial de a grande luta por a implantação de\n",
            " capital Kemal presidiu a grande Assembléia_Nacional .    \n",
            ", seja pequeno , médio ou grande , criar o ambiente adequa\n",
            "          « O Pitarelli é grande ! »                      \n",
            "mas sei que posso ser uma grande atriz .                  \n",
            " « Eu sou aquele oculto e grande cabo / A quem chamais vós\n",
            "vai causar um impacto tão grande em a região que deve ser \n",
            "ropicaliente » parece uma grande aldeia de milionários .  \n",
            "em paga , pode não ser um grande negócio ter de pagar mais\n",
            "ters } destacam- se por a grande resistência .            \n",
            "       Imagino que para a grande maioria de os políticos e\n",
            "            « A lesão foi grande e vai deixar sequelas » .\n",
            "PSDB dizem apostar em uma grande legenda social-democrata \n",
            "conflito judiciário muito grande » , segundo Saulo_Ramos .\n",
            " o problema essencial , a grande delinquência , é a baixa \n",
            "as estão conscientes de a grande responsabilidade que têm \n",
            "         Derlan espera um grande jogo hoje contra o Corint\n",
            "de o Ceará » é também uma grande concentração eleitoral de\n",
            " mineira Contagem é outra grande concentração eleitoral , \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 SpaCy"
      ],
      "metadata": {
        "id": "SF8U6qampqIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# La gran diferencia es que spaCy hace muchas tareas con una sola línea: al aplicar nlp(texto) analiza el texto completo (segmenta, etiqueta, reconoce entidades, etc.)."
      ],
      "metadata": {
        "id": "UnTPSgWPrB7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_en = \"\"\" Arthur Smith bought a Samsung phone in Macau . It\n",
        "was cheaper than in his country . \"\"\"\n",
        "nlp = spacy . load ( \"en_core_web_sm\" )   # Carga el modelo de lenguaje en inglés (con reglas, vocabulario y estadísticas)\n",
        "doc = nlp (text_en)  # Procesa el texto con ese modelo. A partir de aquí, doc contiene información detallada de cada palabra (token).\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_,\n",
        "          token.ent_type_, token.dep_, token.shape_,\n",
        "          token.is_alpha, token.is_stop)\n"
      ],
      "metadata": {
        "id": "Gj5Ydoanps8Y",
        "outputId": "2b3d8926-fbc0-44c6-955a-e22d0f98a761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    SPACE _SP  dep   False False\n",
            "Arthur Arthur PROPN NNP PERSON compound Xxxxx True False\n",
            "Smith Smith PROPN NNP PERSON nsubj Xxxxx True False\n",
            "bought buy VERB VBD  ROOT xxxx True False\n",
            "a a DET DT  det x True True\n",
            "Samsung Samsung PROPN NNP ORG compound Xxxxx True False\n",
            "phone phone NOUN NN  dobj xxxx True False\n",
            "in in ADP IN  prep xx True True\n",
            "Macau Macau PROPN NNP GPE pobj Xxxxx True False\n",
            ". . PUNCT .  punct . False False\n",
            "It it PRON PRP  nsubj Xx True True\n",
            "\n",
            " \n",
            " SPACE _SP  dep \n",
            " False False\n",
            "was be AUX VBD  ROOT xxx True True\n",
            "cheaper cheap ADJ JJR  acomp xxxx True False\n",
            "than than ADP IN  prep xxxx True True\n",
            "in in ADP IN  prep xx True True\n",
            "his his PRON PRP$  poss xxx True True\n",
            "country country NOUN NN  pobj xxxx True False\n",
            ". . PUNCT .  punct . False False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# token.text → texto original\n",
        "# token.lemma_ → forma base\n",
        "# token.pos_ → categoría gramatical\n",
        "# token.ent_type_ → tipo de entidad (si aplica)\n",
        "# token.dep_ → rol sintáctico\n",
        "# token.shape_ → forma (mayúsculas, dígitos...)\n",
        "# token.is_alpha → si son solo letras\n",
        "# token.is_stop → si es una stopword"
      ],
      "metadata": {
        "id": "GKwJReuarUDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJEMPLO 1.6"
      ],
      "metadata": {
        "id": "g0OIKZ9os3f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧠 Objetivo del ejemplo\n",
        "# Mostrar cómo spaCy puede analizar texto en portugués simplemente cambiando el modelo de idioma cargado (de inglés a portugués)."
      ],
      "metadata": {
        "id": "tmGvrJxes2uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm\n",
        "sentence = \"\"\"O Artur Mendes comprou um telefone Samsung em Macau.\"\"\"\n",
        "nlp = spacy.load(\"pt_core_news_sm\")     # 🔹 Carga el modelo de spaCy en portugués.🔹 pt = portugués 🔹 core_news = modelo general de noticias 🔹 sm = versión pequeña (small)\n",
        "doc = nlp(sentence)\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_,\n",
        "          token.ent_type_, token.dep_, token.shape_,\n",
        "          token.is_alpha, token.is_stop)"
      ],
      "metadata": {
        "id": "oso7PiSus__t",
        "outputId": "110026c5-4d84-418b-b87d-d948c63c3d84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "O o DET DET  det X True True\n",
            "Artur Artur PROPN PROPN PER nsubj Xxxxx True False\n",
            "Mendes Mendes PROPN PROPN PER flat:name Xxxxx True False\n",
            "comprou comprar VERB VERB  ROOT xxxx True False\n",
            "um um DET DET  det xx True True\n",
            "telefone telefone NOUN NOUN  obj xxxx True False\n",
            "Samsung Samsung PROPN PROPN ORG appos Xxxxx True False\n",
            "em em ADP ADP  case xx True True\n",
            "Macau Macau PROPN PROPN LOC nmod Xxxxx True False\n",
            ". . PUNCT PUNCT  punct . False False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJERCICIO 1.3"
      ],
      "metadata": {
        "id": "RtGb1-_9uGdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 🧠 Exercício 1.3 – spaCy em Português\n",
        "# ============================================\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Cargar el modelo en portugués\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# 🧩 Exemplo 1: A mesma palavra com categorias gramaticais diferentes\n",
        "# --------------------------------------------\n",
        "text1 = \"Ele gosta de correr todos os dias. O corredor é rápido.\"\n",
        "doc1 = nlp(text1)\n",
        "\n",
        "print(\"=== Exemplo 1: Mesma palavra com diferentes categorias gramaticais ===\")\n",
        "print(f\"{'TOKEN':<15} {'POS':<10} {'LEMMA':<15}\")\n",
        "print(\"-\" * 40)\n",
        "for token in doc1:\n",
        "    print(f\"{token.text:<15} {token.pos_:<10} {token.lemma_:<15}\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# 🧩 Exemplo 2: A mesma forma com lemas diferentes\n",
        "# --------------------------------------------\n",
        "text2 = \"Ele foi ao banco descansar. O banco estava fechado.\"\n",
        "doc2 = nlp(text2)\n",
        "\n",
        "print(\"\\n=== Exemplo 2: Mesma forma com lemas diferentes ===\")\n",
        "print(f\"{'TOKEN':<15} {'POS':<10} {'LEMMA':<15}\")\n",
        "print(\"-\" * 40)\n",
        "for token in doc2:\n",
        "    print(f\"{token.text:<15} {token.pos_:<10} {token.lemma_:<15}\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# 💡 Observações:\n",
        "# - No Exemplo 1, “correr” aparece como verbo (gosta de correr)\n",
        "#   e como substantivo (O correr é saudável, se quiseres testar).\n",
        "# - No Exemplo 2, “banco” tem o mesmo texto, mas spaCy pode interpretá-lo\n",
        "#   de formas diferentes dependendo do contexto (instituição financeira ou objeto físico).\n",
        "# --------------------------------------------\n"
      ],
      "metadata": {
        "id": "o0D1divVuIPi",
        "outputId": "3d6a671a-3864-46d2-b5b5-0c62d4e66098",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Exemplo 1: Mesma palavra com diferentes categorias gramaticais ===\n",
            "TOKEN           POS        LEMMA          \n",
            "----------------------------------------\n",
            "Ele             PRON       ele            \n",
            "gosta           VERB       gostar         \n",
            "de              SCONJ      de             \n",
            "correr          VERB       correr         \n",
            "todos           DET        todo           \n",
            "os              ADP        os             \n",
            "dias            NOUN       dias           \n",
            ".               PUNCT      .              \n",
            "O               DET        o              \n",
            "corredor        NOUN       corredor       \n",
            "é               AUX        ser            \n",
            "rápido          ADJ        rápido         \n",
            ".               PUNCT      .              \n",
            "\n",
            "=== Exemplo 2: Mesma forma com lemas diferentes ===\n",
            "TOKEN           POS        LEMMA          \n",
            "----------------------------------------\n",
            "Ele             PRON       ele            \n",
            "foi             VERB       ir             \n",
            "ao              ADP        a o            \n",
            "banco           NOUN       banco          \n",
            "descansar       VERB       descansar      \n",
            ".               PUNCT      .              \n",
            "O               DET        o              \n",
            "banco           NOUN       banco          \n",
            "estava          AUX        estar          \n",
            "fechado         ADJ        fechado        \n",
            ".               PUNCT      .              \n"
          ]
        }
      ]
    }
  ]
}