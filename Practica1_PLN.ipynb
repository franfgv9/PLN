{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franfgv9/PLN/blob/main/Practica1_PLN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Arthur Smith bought a Samsung phone in Macau . It was cheaperthan in his country . \"\"\""
      ],
      "metadata": {
        "id": "wjPbt6kPHbnP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK"
      ],
      "metadata": {
        "id": "wYGGrvOMFlxQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyWhyt1uE6XF",
        "outputId": "39498c59-c5d0-45ed-e4f0-e44f0241bb95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "# data required for the examples and exercises in the book\n",
        "nltk . download ( \"book\")\n",
        "nltk . download ('punkt_tab')\n",
        "nltk . download ('averaged_perceptron_tagger_eng')\n",
        "nltk . download ('maxent_ne_chunker_tab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy\n"
      ],
      "metadata": {
        "id": "IUMH2vswFpt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "O2ZqUDB3FuLW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJEMPLO 1.1"
      ],
      "metadata": {
        "id": "e9PJsEqiGTc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizar un texto en frases\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1uuQ66PGVC6",
        "outputId": "d5a33228-dc44-4930-ecb5-6a572b254fca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Arthur Smith bought a Samsung phone in Macau .', 'It was cheaperthan in his country .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizar cada frase del texto en palabras\n",
        "\n",
        "for sent in sentences:\n",
        "  tokens = nltk.word_tokenize(sent)\n",
        "  print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3Vh2oLvGmDh",
        "outputId": "5a7c9474-4788-4993-c4ce-5f59137f0862"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Arthur', 'Smith', 'bought', 'a', 'Samsung', 'phone', 'in', 'Macau', '.']\n",
            "['It', 'was', 'cheaperthan', 'in', 'his', 'country', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Te da los verbos y su POS tagset (si es un verbo, adjetivo, etc)\n",
        "\n",
        "tagged = nltk . pos_tag ( tokens )\n",
        "print ( tagged )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwNpk_OFfXrh",
        "outputId": "fc5897ff-8922-4fd7-e902-e022b596bbc1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('It', 'PRP'), ('was', 'VBD'), ('cheaperthan', 'VBN'), ('in', 'IN'), ('his', 'PRP$'), ('country', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entities = nltk . chunk . ne_chunk ( tagged )\n",
        "print ( entities )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeLhwHyDfbqE",
        "outputId": "2c4e12dc-e578-48bd-844d-5b1498e9a7af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S It/PRP was/VBD cheaperthan/VBN in/IN his/PRP$ country/NN ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exerc√≠cio 1.2"
      ],
      "metadata": {
        "id": "gBU6-fDfK2Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk . corpus import floresta\n",
        "nltk . download ('floresta')"
      ],
      "metadata": {
        "id": "wBvLbG4RK26D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2521b96-6141-440c-dd89-126709470ae7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/floresta.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de palavras\n",
        "words = floresta . words ()\n",
        "# Lista de frases tokenizadas\n",
        "sentences = floresta . sents ()"
      ],
      "metadata": {
        "id": "u_HK05nOhpGB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para imprimir el numero de palabras (tokens) y el numero de los distintos tipos de palabras\n",
        "\n",
        "fd = nltk . FreqDist ( words )\n",
        "print ('Tokens', len ( words ) )\n",
        "print ('Types', len ( fd ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_bhy2bLhwxI",
        "outputId": "024b03ab-d1fa-4922-a1f3-b6214944223f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens 211852\n",
            "Types 29421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para imprimir las palabras m√°s frecuentes\n",
        "\n",
        "print ('Most frequent word ', fd . max () )\n",
        "print ('10 most frequent words ', fd . most_common (10) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3veKumxiBi9",
        "outputId": "770e6ddc-17c6-4902-c57a-2478e36cc288"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent word  de\n",
            "10 most frequent words  [('de', 14569), (',', 13444), ('a', 12656), ('o', 10025), ('.', 7725), ('em', 5505), ('e', 3981), ('que', 3956), ('os', 3223), ('¬´', 2369)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJERCICIO 1"
      ],
      "metadata": {
        "id": "JYzIRxz1iYnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopwords --> palabras irrelevantes (el, a, que, etc)\n",
        "import string\n",
        "\n",
        "# Carga la lista de stopwords en portugu√©s usando NLTK.\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "# print(stopwords[:10])\n",
        "\n",
        "fd = nltk.FreqDist(w.lower() for w in floresta.words()\n",
        "                   if w.lower() not in stopwords and w not in string.punctuation)\n",
        "most_frequent = fd.most_common(20)\n",
        "for w in most_frequent:\n",
        "    print(w)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6CYncctiTfv",
        "outputId": "d687b671-4f23-4be7-872d-4cf63e485354"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', '√†', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJERCICIO 1.2"
      ],
      "metadata": {
        "id": "W1Wa1Jt0l2GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† Objetivo del c√≥digo\n",
        "  # El prop√≥sito es reducir las palabras a su ra√≠z (stem) antes de calcular su frecuencia.\n",
        "  # Esto permite agrupar palabras que tienen el mismo significado base, aunque est√©n en distintas formas (g√©nero, n√∫mero, conjugaci√≥n‚Ä¶)."
      ],
      "metadata": {
        "id": "edtarZTMmv9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Un stemmer corta las palabras hasta su ra√≠z o base com√∫n.\n",
        "nltk . download ('rslp')\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "print ( stemmer . stem ('grande') )\n",
        "print ( stemmer . stem ('grandes') )\n",
        "print ( stemmer . stem ('grandinha') )\n",
        "print ( stemmer . stem ('grandalh√£o') )\n",
        "print ( stemmer . stem ('ser') )\n",
        "print ( stemmer . stem ('sou') )\n",
        "print ( stemmer . stem ('fui') )\n",
        "\n",
        "print('FLORESTA ####')\n",
        "fd = nltk.FreqDist(stemmer.stem(w.lower())\n",
        "                   for w in floresta.words()\n",
        "                   if w.lower() not in stopwords and w not in string.punctuation)\n",
        "\n",
        "most_frequent = fd.most_common(20)\n",
        "for mf in most_frequent:\n",
        "    print(mf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0tr4HL6l4TF",
        "outputId": "e90ea835-d701-445d-9124-444f5185e3ea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grand\n",
            "grand\n",
            "grand\n",
            "grand\n",
            "ser\n",
            "sou\n",
            "fui\n",
            "FLORESTA ####\n",
            "('¬´', 2369)\n",
            "('¬ª', 2310)\n",
            "('ano', 491)\n",
            "('pod', 445)\n",
            "('pass', 361)\n",
            "('outr', 354)\n",
            "('sobr', 354)\n",
            "('segund', 319)\n",
            "('nov', 319)\n",
            "('prim', 300)\n",
            "('dia', 299)\n",
            "('ont', 292)\n",
            "('aind', 279)\n",
            "('part', 278)\n",
            "('ter', 270)\n",
            "('cas', 262)\n",
            "('dev', 260)\n",
            "('govern', 254)\n",
            "('faz', 248)\n",
            "('jog', 247)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJEMPLO 1.4"
      ],
      "metadata": {
        "id": "tyJC52W4nmB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta funci√≥n busca una palabra dentro del corpus y muestra c√≥mo aparece en su contexto, es decir, qu√© palabras tiene alrededor."
      ],
      "metadata": {
        "id": "_-n4FgGVpHeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concordance ( corpus , word , context =30) :\n",
        "  for sent in corpus . sents () :\n",
        "    if word in sent :\n",
        "      pos = sent . index ( word )  # Obtiene la posici√≥n (√≠ndice) de la palabra dentro de la oraci√≥n\n",
        "      left = ' '. join ( sent [: pos ])   # : todas las palabras que hay antes de la palabra buscada\n",
        "      right = ' '. join ( sent [ pos +1:])\n",
        "      print ('%*s %s %-*s' % ( context , left [ - context :] , word , context , right [:context ]) )"
      ],
      "metadata": {
        "id": "DiMW2UFknn2h"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concordance(floresta,'grande', context=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XZDVwqnn6Ja",
        "outputId": "e9d8edfb-bdc3-4402-9843-dfaf3351e62b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " de a equipa n√£o ajudasse grande coisa .                  \n",
            " igualdade atrav√©s de uma grande penalidade marcada por Ru\n",
            " os √∫ltimos 24 meses em a grande Casa_Comum_Europeia .    \n",
            " durante toda_a noite , o grande veleiro foi tomado de ass\n",
            "e ele acaba por ir- se em grande dificuldade .            \n",
            "carros estacionados , uma grande quantidade de pequenos le\n",
            "Guimar√£es est√° a suscitar grande interesse em a regi√£o min\n",
            "tos de o pa√≠s ¬ª exigem um grande controlo de os gastos .  \n",
            "           Mas o primeiro grande atleta de Moniz_Pereira v\n",
            "√≠s que t√™m uma atitude de grande dignidade por os animais \n",
            "mpos bateu patinadoras de grande nome como a alem√£ Tanja_S\n",
            "                      E a grande d√∫vida √© se as suspeitas \n",
            "menos em_rela√ß√£o_a aquele grande estadista , que penso ser\n",
            "riedade de os edif√≠cios , grande parte de eles foram ocupa\n",
            "        Percebe- se que a grande massa de gente est√° ali m\n",
            "inas de filmar for√ßaram o grande √™xodo .                  \n",
            "at√© a o fim de a primeira grande guerra dirigiu e supervis\n",
            "rte portuguesa de a nossa grande equipa tem sido eficaz e \n",
            " que podem significar uma grande revolu√ß√£o para o mundo de\n",
            "                        A grande quest√£o √© saber se havia \n",
            "uca√ß√£o , que v√™ em ele um grande futuro .                 \n",
            "obiliza√ß√£o , pois ¬´ o seu grande intuito foi despolitizar \n",
            " A Uni√£o_Nacional era uma grande frente onde Salazar toler\n",
            "qualquer , de_prefer√™ncia grande .                        \n",
            ", onde McStay se exibia a grande altura .                 \n",
            "lega√ß√£o nenhuma , uma t√£o grande aflu√™ncia de s√≥cios para \n",
            "ter-Fiorentina era o jogo grande de a terceira jornada de \n",
            "               Uma medida grande para o seu corpo pesado ,\n",
            "      Um aviso fugaz em o grande mercado de as aves , quan\n",
            " , ter√° dois encontros de grande emotividade ;            \n",
            "fe_do_Estoril continua em grande ;                        \n",
            "  Motor ; Econ√≥mico , com grande elasticidade e bom n√≠vel \n",
            "es de Maastricht ¬´ s√£o de grande import√¢ncia para o pa√≠s ¬ª\n",
            "s de rotina a os museus , grande parte de os washingtonian\n",
            " Lisboa , 1994 } leve uma grande volta .                  \n",
            "                        O grande √™xito de o Vaticano em a \n",
            "                    Mas o grande motivo de o atraso em o a\n",
            "  Esta √© ali√°s a primeira grande surpresa de 1994 , que a \n",
            "nto de a morte de o seu ¬´ grande amigo ¬ª , evocou um encon\n",
            "scender a o para√≠so , t√£o grande √© a beleza celestial de a\n",
            " um grau de incerteza t√£o grande que , a o ser accionada ,\n",
            "nciou o pr√©-programa de a grande celebra√ß√£o de os estudant\n",
            "assado , apresent√°mos com grande sucesso v√°rios conjuntos \n",
            "Desde_logo por_causa_de o grande afluxo de p√∫blico        \n",
            "r , para essa gera√ß√£o , a grande escola inici√°tica para qu\n",
            " Alberto_Jo√£o , n√£o era ¬´ grande espingarda ¬ª e ¬´ antip√°ti\n",
            " o ministro j√° manifestou grande curiosidade quanto_a o qu\n",
            "correndo o seu flanco com grande √†-vontade .              \n",
            "fastadas ; arquitectura e grande p√∫blico .                \n",
            "                        A grande d√∫vida que circula entre \n",
            "a- se- uma recupera√ß√£o de grande categoria , em a_qual alc\n",
            "s anos 40 ¬ª renovando ¬´ a grande tradi√ß√£o de Hammet e Chan\n",
            " o referendo dentro_de um grande envelope .               \n",
            "mia familiar s√£o ainda em grande n√∫mero , segundo informou\n",
            " √© considerada o primeiro grande instrumento de trabalho e\n",
            "o Vaticano ou provoca uma grande pol√©mica e torna- se um s\n",
            "e os 85 ienes e mostrando grande firmeza .                \n",
            "ou um pneu e ouviu- se um grande estrondo .               \n",
            "                     Jogo grande foi o que juntou , em Pho\n",
            "ara em alguns momentos de grande pol√©mica .               \n",
            "mos no_caso_de √Åfrica , a grande quantidade de gente que a\n",
            " notou que ¬´ era a irm√£ o grande apoio em os estudos ¬ª , c\n",
            "dependente de uma obra de grande envergadura , ainda sem l\n",
            "bem , n√£o parecem ter uma grande cultura pr√≥pria .        \n",
            "recurso a uma mira , √© de grande utilidade .              \n",
            "   De esse dia conserva o grande recorte de jornal e uma c\n",
            "como o diz , ter√° sido um grande al√≠vio .                 \n",
            "              Ainda_que a grande preocupa√ß√£o de o p√∫blico \n",
            "jectos de expans√£o { para grande dor-de-cabe√ßa de as marca\n",
            "dades nacionais n√£o viram grande receptividade e decidiram\n",
            " e que reenvia para a sua grande escultura de ninfas com u\n",
            " de poupadores de m√©dio e grande porte em_tese , menos suj\n",
            "         ¬´ O Helmet √© uma grande se√ß√£o r√≠tmica ¬ª , definiu\n",
            "ho de sete cabe√ßas ¬ª para grande parte de os estudantes de\n",
            "                     √â um grande pintor , e sua √∫ltima fas\n",
            "o s√£o ramifica√ß√µes de uma grande organiza√ß√£o criminosa .  \n",
            "gres ¬ª seria recebido sem grande estardalha√ßo .           \n",
            "                 Essa √© a grande novidade de o Carnaval_94\n",
            "     A agita√ß√£o ontem foi grande tamb√©m em a Bolsa_de_Merc\n",
            " sinto uma ang√∫stia muito grande ¬ª , afirmou .            \n",
            "ntes de as torcidas de os grande times de S√£o_Paulo { Cori\n",
            "o vou saber se meu amor √© grande o suficiente ? ¬ª         \n",
            "sta , porque o amor n√£o √© grande nem pequeno .            \n",
            "                        A grande vantagem de os fundos de \n",
            "a parte mais crucial de a grande luta por a implanta√ß√£o de\n",
            " capital Kemal presidiu a grande Assembl√©ia_Nacional .    \n",
            ", seja pequeno , m√©dio ou grande , criar o ambiente adequa\n",
            "          ¬´ O Pitarelli √© grande ! ¬ª                      \n",
            "mas sei que posso ser uma grande atriz .                  \n",
            " ¬´ Eu sou aquele oculto e grande cabo / A quem chamais v√≥s\n",
            "vai causar um impacto t√£o grande em a regi√£o que deve ser \n",
            "ropicaliente ¬ª parece uma grande aldeia de milion√°rios .  \n",
            "em paga , pode n√£o ser um grande neg√≥cio ter de pagar mais\n",
            "ters } destacam- se por a grande resist√™ncia .            \n",
            "       Imagino que para a grande maioria de os pol√≠ticos e\n",
            "            ¬´ A les√£o foi grande e vai deixar sequelas ¬ª .\n",
            "PSDB dizem apostar em uma grande legenda social-democrata \n",
            "conflito judici√°rio muito grande ¬ª , segundo Saulo_Ramos .\n",
            " o problema essencial , a grande delinqu√™ncia , √© a baixa \n",
            "as est√£o conscientes de a grande responsabilidade que t√™m \n",
            "         Derlan espera um grande jogo hoje contra o Corint\n",
            "de o Cear√° ¬ª √© tamb√©m uma grande concentra√ß√£o eleitoral de\n",
            " mineira Contagem √© outra grande concentra√ß√£o eleitoral , \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 SpaCy"
      ],
      "metadata": {
        "id": "SF8U6qampqIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# La gran diferencia es que spaCy hace muchas tareas con una sola l√≠nea: al aplicar nlp(texto) analiza el texto completo (segmenta, etiqueta, reconoce entidades, etc.)."
      ],
      "metadata": {
        "id": "UnTPSgWPrB7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_en = \"\"\" Arthur Smith bought a Samsung phone in Macau . It\n",
        "was cheaper than in his country . \"\"\"\n",
        "nlp = spacy . load ( \"en_core_web_sm\" )   # Carga el modelo de lenguaje en ingl√©s (con reglas, vocabulario y estad√≠sticas)\n",
        "doc = nlp (text_en)  # Procesa el texto con ese modelo. A partir de aqu√≠, doc contiene informaci√≥n detallada de cada palabra (token).\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_,\n",
        "          token.ent_type_, token.dep_, token.shape_,\n",
        "          token.is_alpha, token.is_stop)\n"
      ],
      "metadata": {
        "id": "Gj5Ydoanps8Y",
        "outputId": "2b3d8926-fbc0-44c6-955a-e22d0f98a761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    SPACE _SP  dep   False False\n",
            "Arthur Arthur PROPN NNP PERSON compound Xxxxx True False\n",
            "Smith Smith PROPN NNP PERSON nsubj Xxxxx True False\n",
            "bought buy VERB VBD  ROOT xxxx True False\n",
            "a a DET DT  det x True True\n",
            "Samsung Samsung PROPN NNP ORG compound Xxxxx True False\n",
            "phone phone NOUN NN  dobj xxxx True False\n",
            "in in ADP IN  prep xx True True\n",
            "Macau Macau PROPN NNP GPE pobj Xxxxx True False\n",
            ". . PUNCT .  punct . False False\n",
            "It it PRON PRP  nsubj Xx True True\n",
            "\n",
            " \n",
            " SPACE _SP  dep \n",
            " False False\n",
            "was be AUX VBD  ROOT xxx True True\n",
            "cheaper cheap ADJ JJR  acomp xxxx True False\n",
            "than than ADP IN  prep xxxx True True\n",
            "in in ADP IN  prep xx True True\n",
            "his his PRON PRP$  poss xxx True True\n",
            "country country NOUN NN  pobj xxxx True False\n",
            ". . PUNCT .  punct . False False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# token.text ‚Üí texto original\n",
        "# token.lemma_ ‚Üí forma base\n",
        "# token.pos_ ‚Üí categor√≠a gramatical\n",
        "# token.ent_type_ ‚Üí tipo de entidad (si aplica)\n",
        "# token.dep_ ‚Üí rol sint√°ctico\n",
        "# token.shape_ ‚Üí forma (may√∫sculas, d√≠gitos...)\n",
        "# token.is_alpha ‚Üí si son solo letras\n",
        "# token.is_stop ‚Üí si es una stopword"
      ],
      "metadata": {
        "id": "GKwJReuarUDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJEMPLO 1.6"
      ],
      "metadata": {
        "id": "g0OIKZ9os3f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† Objetivo del ejemplo\n",
        "# Mostrar c√≥mo spaCy puede analizar texto en portugu√©s simplemente cambiando el modelo de idioma cargado (de ingl√©s a portugu√©s)."
      ],
      "metadata": {
        "id": "tmGvrJxes2uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm\n",
        "sentence = \"\"\"O Artur Mendes comprou um telefone Samsung em Macau.\"\"\"\n",
        "nlp = spacy.load(\"pt_core_news_sm\")     # üîπ Carga el modelo de spaCy en portugu√©s.üîπ pt = portugu√©s üîπ core_news = modelo general de noticias üîπ sm = versi√≥n peque√±a (small)\n",
        "doc = nlp(sentence)\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_,\n",
        "          token.ent_type_, token.dep_, token.shape_,\n",
        "          token.is_alpha, token.is_stop)"
      ],
      "metadata": {
        "id": "oso7PiSus__t",
        "outputId": "110026c5-4d84-418b-b87d-d948c63c3d84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "O o DET DET  det X True True\n",
            "Artur Artur PROPN PROPN PER nsubj Xxxxx True False\n",
            "Mendes Mendes PROPN PROPN PER flat:name Xxxxx True False\n",
            "comprou comprar VERB VERB  ROOT xxxx True False\n",
            "um um DET DET  det xx True True\n",
            "telefone telefone NOUN NOUN  obj xxxx True False\n",
            "Samsung Samsung PROPN PROPN ORG appos Xxxxx True False\n",
            "em em ADP ADP  case xx True True\n",
            "Macau Macau PROPN PROPN LOC nmod Xxxxx True False\n",
            ". . PUNCT PUNCT  punct . False False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EJERCICIO 1.3"
      ],
      "metadata": {
        "id": "RtGb1-_9uGdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üß† Exerc√≠cio 1.3 ‚Äì spaCy em Portugu√™s\n",
        "# ============================================\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Cargar el modelo en portugu√©s\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# üß© Exemplo 1: A mesma palavra com categorias gramaticais diferentes\n",
        "# --------------------------------------------\n",
        "text1 = \"Ele gosta de correr todos os dias. O corredor √© r√°pido.\"\n",
        "doc1 = nlp(text1)\n",
        "\n",
        "print(\"=== Exemplo 1: Mesma palavra com diferentes categorias gramaticais ===\")\n",
        "print(f\"{'TOKEN':<15} {'POS':<10} {'LEMMA':<15}\")\n",
        "print(\"-\" * 40)\n",
        "for token in doc1:\n",
        "    print(f\"{token.text:<15} {token.pos_:<10} {token.lemma_:<15}\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# üß© Exemplo 2: A mesma forma com lemas diferentes\n",
        "# --------------------------------------------\n",
        "text2 = \"Ele foi ao banco descansar. O banco estava fechado.\"\n",
        "doc2 = nlp(text2)\n",
        "\n",
        "print(\"\\n=== Exemplo 2: Mesma forma com lemas diferentes ===\")\n",
        "print(f\"{'TOKEN':<15} {'POS':<10} {'LEMMA':<15}\")\n",
        "print(\"-\" * 40)\n",
        "for token in doc2:\n",
        "    print(f\"{token.text:<15} {token.pos_:<10} {token.lemma_:<15}\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# üí° Observa√ß√µes:\n",
        "# - No Exemplo 1, ‚Äúcorrer‚Äù aparece como verbo (gosta de correr)\n",
        "#   e como substantivo (O correr √© saud√°vel, se quiseres testar).\n",
        "# - No Exemplo 2, ‚Äúbanco‚Äù tem o mesmo texto, mas spaCy pode interpret√°-lo\n",
        "#   de formas diferentes dependendo do contexto (institui√ß√£o financeira ou objeto f√≠sico).\n",
        "# --------------------------------------------\n"
      ],
      "metadata": {
        "id": "o0D1divVuIPi",
        "outputId": "3d6a671a-3864-46d2-b5b5-0c62d4e66098",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Exemplo 1: Mesma palavra com diferentes categorias gramaticais ===\n",
            "TOKEN           POS        LEMMA          \n",
            "----------------------------------------\n",
            "Ele             PRON       ele            \n",
            "gosta           VERB       gostar         \n",
            "de              SCONJ      de             \n",
            "correr          VERB       correr         \n",
            "todos           DET        todo           \n",
            "os              ADP        os             \n",
            "dias            NOUN       dias           \n",
            ".               PUNCT      .              \n",
            "O               DET        o              \n",
            "corredor        NOUN       corredor       \n",
            "√©               AUX        ser            \n",
            "r√°pido          ADJ        r√°pido         \n",
            ".               PUNCT      .              \n",
            "\n",
            "=== Exemplo 2: Mesma forma com lemas diferentes ===\n",
            "TOKEN           POS        LEMMA          \n",
            "----------------------------------------\n",
            "Ele             PRON       ele            \n",
            "foi             VERB       ir             \n",
            "ao              ADP        a o            \n",
            "banco           NOUN       banco          \n",
            "descansar       VERB       descansar      \n",
            ".               PUNCT      .              \n",
            "O               DET        o              \n",
            "banco           NOUN       banco          \n",
            "estava          AUX        estar          \n",
            "fechado         ADJ        fechado        \n",
            ".               PUNCT      .              \n"
          ]
        }
      ]
    }
  ]
}