{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMp+cOIJmJfElyP6fATakGq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franfgv9/PLN/blob/main/Practica_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5KhW5wcJ9CL",
        "outputId": "65aa4af9-1c7a-4fdb-c369-a48298e86160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-05 22:53:51--  https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/proverbios_N.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68195 (67K) [text/plain]\n",
            "Saving to: ‘proverbios.txt’\n",
            "\n",
            "proverbios.txt      100%[===================>]  66.60K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-10-05 22:53:51 (3.51 MB/s) - ‘proverbios.txt’ saved [68195/68195]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/proverbios_N.txt -O proverbios.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "El objetivo inicial es crear o cargar una colección de documentos de texto y representarlos como una lista de cadenas de texto en Python.\n",
        "Cada línea de texto del fichero representará un documento (por ejemplo, un proverbio o una frase).\n",
        "\n",
        "🧠 Explicación teórica\n",
        "\n",
        "El primer paso en cualquier proyecto de procesamiento de lenguaje natural es obtener los datos textuales."
      ],
      "metadata": {
        "id": "-3fLPnfUkLQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentario = '#'\n",
        "def carrega_docs ( ficheiro ) :\n",
        "  docs = []\n",
        "  with open ( ficheiro , encoding ='utf -8 ') as f :\n",
        "    lines = f . readlines ()\n",
        "    for txt in lines :\n",
        "      txt = txt . strip ()\n",
        "      if len ( txt ) > 0 and txt [0] != comentario :\n",
        "        docs . append ( txt )\n",
        "  return docs\n",
        "\n",
        "# Se abre el fichero (open(ficheiro, encoding='utf-8'))\n",
        "# Se leen todas las líneas (readlines())\n",
        "# Se limpian los espacios y saltos de línea (strip())\n",
        "# Se ignoran comentarios o líneas vacías\n",
        "# Finalmente, se devuelve una lista de documentos, donde cada elemento es una cadena de texto (un documento independiente).\n"
      ],
      "metadata": {
        "id": "7GtaW4QFU6Vr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Exemplo 1.2 – Bag of Words (Bolsa de Palavras)"
      ],
      "metadata": {
        "id": "5Zz5lyzNW-PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "Transformar una colección de textos en una matriz numérica que indique cuántas veces aparece cada palabra en cada documento.\n",
        "Esta representación es fundamental porque los algoritmos de Machine Learning no pueden trabajar directamente con texto: necesitan números.\n",
        "\n",
        "\n",
        "🧠 Explicación teórica\n",
        "\n",
        "El modelo Bag-of-Words (BoW) consiste en:\n",
        "\n",
        "  1. Crear un vocabulario con todas las palabras únicas del corpus.\n",
        "\n",
        "  2. Representar cada documento como un vector de frecuencias de esas palabras.\n",
        "\n",
        "📊 Cada fila = un documento\n",
        "📈 Cada columna = una palabra del vocabulario\n",
        "💾 Cada valor = número de veces que la palabra aparece en el documento"
      ],
      "metadata": {
        "id": "pPyhq46-ksdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Cargamos los documentos (por ejemplo, los proverbios)\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador\n",
        "c_vect = CountVectorizer()\n",
        "\n",
        "# Transformamos los documentos en una matriz documento × término\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Visualizamos la forma de la matriz (nº de documentos, nº de palabras)\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "\n",
        "# Obtenemos el vocabulario (todas las palabras únicas)\n",
        "print(\"Vocabulario:\", c_vect.get_feature_names_out())\n",
        "\n",
        "# Visualizamos el vector del primer documento\n",
        "print(\"Representação do primeiro documento:\")\n",
        "print(p_counts[0, :].toarray().tolist())\n",
        "print(p_counts[0, :])\n",
        "print(docs[0])\n"
      ],
      "metadata": {
        "id": "tD6JqH3JluCL",
        "outputId": "92e09faf-a4ce-4cd1-da0b-a04562ee1578",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 2741)\n",
            "Vocabulario: ['abafadiço' 'abaixa' 'abaixo' ... 'último' 'últimos' 'útil']\n",
            "Representação do primeiro documento:\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 4 stored elements and shape (1, 2741)>\n",
            "  Coords\tValues\n",
            "  (0, 7)\t1\n",
            "  (0, 1767)\t1\n",
            "  (0, 1430)\t1\n",
            "  (0, 546)\t1\n",
            "a abelha não leva chumbo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Exemplo 1.3 – N-gramas com CountVectorizer"
      ],
      "metadata": {
        "id": "4rrCX0Z3XABF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "Mejorar la representación del texto considerando no solo palabras individuales, sino también secuencias de palabras consecutivas (los llamados n-gramas).\n",
        "Esto permite capturar expresiones y contextos que el modelo Bag-of-Words simple no detecta.\n",
        "\n",
        "🔍 ¿Por qué usar n-gramas?\n",
        "\n",
        "Unigramas capturan las palabras por separado (útiles para vocabularios amplios\n",
        "\n",
        "Bigramas o trigramas capturan contexto local:\n",
        "\n",
        "  “não gosto” ≠ “gosto”\n",
        "  \n",
        "  “muito bom” → sentimiento positivo que una sola palabra no revela."
      ],
      "metadata": {
        "id": "yNajtNcdnzFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador con n-gramas (de 1 a 3 palabras)\n",
        "c_vect = CountVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "# Representamos los documentos en una matriz documento × término\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Mostramos información sobre la matriz generada\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "print(\"Número de documentos:\", p_counts.shape[0])\n",
        "print(\"Tamanho do vocabulário (features):\", p_counts.shape[1])\n",
        "\n",
        "# Mostramos algunas features detectadas\n",
        "print(\"\\nExemplos de features (n-gramas detectados):\")\n",
        "print(list(c_vect.get_feature_names_out())[:30])  # solo los 30 primeros\n",
        "\n",
        "# COMENTARIO: logicamente ahora vemos mas vocabulario que antes porque ahora al tener n-gramas tenemos mas combinaicones de palabras nuevas"
      ],
      "metadata": {
        "id": "FPJhAOWFoYEU",
        "outputId": "c8cd8492-c321-49bd-fcb3-93f2a8a2c836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 17120)\n",
            "Número de documentos: 1617\n",
            "Tamanho do vocabulário (features): 17120\n",
            "\n",
            "Exemplos de features (n-gramas detectados):\n",
            "['abafadiço', 'abafadiço sai', 'abafadiço sai abelha', 'abaixa', 'abaixa rabo', 'abaixa rabo se', 'abaixo', 'abaixo come', 'abandona', 'abarca', 'abarca pouco', 'abarca pouco abraça', 'abastado', 'abastado quem', 'abastado quem dá', 'abegão', 'abelha', 'abelha do', 'abelha do cortiço', 'abelha não', 'abelha não leva', 'abelha senão', 'abelha senão quem', 'abelhas', 'abelhas para', 'abelhas para as', 'abençoada', 'aberta', 'aberta ou', 'aberta ou entra']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Exemplo 1.4 – Ajustes avançados em CountVectorizer"
      ],
      "metadata": {
        "id": "fj2-yRnYY2Pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_features:\tDefine un número máximo de palabras a conservar. Usa las más frecuentes.\t  max_features=1000 mantiene solo las 1000 palabras más comunes.\n",
        "\n",
        "max_df:\tPalabras que aparecen en demasiados documentos se descartan. Puede ser valor absoluto o proporción.\t max_df=0.75 ignora palabras que aparecen en ≥75 % de los documentos.\n",
        "\n",
        "min_df:\tPalabras que aparecen muy poco también se descartan.\tmin_df=5 mantiene solo palabras que aparecen en ≥5 documentos.\n",
        "\n",
        "strip_accents:\tElimina acentos antes del análisis.\t\"coração\" → \"coracao\"\n",
        "\n",
        "stop_words:\tPermite eliminar stopwords (palabras vacías sin significado, como “o”, “de”, “em”).\tSe puede usar la lista de stopwords del idioma portugués en NLTK."
      ],
      "metadata": {
        "id": "Au7ZeZNupkRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "\n",
        "# Descargamos las stopwords de NLTK (solo la primera vez)\n",
        "nltk.download('stopwords')\n",
        "pt_stop = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador con parámetros personalizados\n",
        "c_vect = CountVectorizer(\n",
        "    ngram_range=(1, 3),       # unigramas, bigramas y trigramas\n",
        "    strip_accents='unicode',  # elimina acentos\n",
        "    stop_words=pt_stop,       # usa stopwords en portugués\n",
        "    min_df=5,                 # ignora palabras que aparecen en <5 documentos\n",
        "    max_df=0.75,              # ignora palabras que aparecen en >75% de documentos\n",
        "    max_features=1000         # mantiene las 1000 más frecuentes\n",
        ")\n",
        "\n",
        "# Generamos la matriz documento × término\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Información sobre la matriz\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "print(\"Número de features finais:\", len(c_vect.get_feature_names_out()))\n",
        "\n",
        "# Visualizamos algunas features seleccionadas\n",
        "print(\"\\nAlgumas features seleccionadas:\")\n",
        "print(list(c_vect.get_feature_names_out())[:30])\n"
      ],
      "metadata": {
        "id": "K5-IldQiqc_w",
        "outputId": "7b20ccd0-a2be-44c3-f3c0-e045b06fdc6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ate', 'eramos', 'estao', 'estavamos', 'estiveramos', 'estivessemos', 'foramos', 'fossemos', 'ha', 'hao', 'houveramos', 'houverao', 'houveriamos', 'houvessemos', 'ja', 'nao', 'sao', 'sera', 'serao', 'seriamos', 'so', 'tambem', 'tera', 'terao', 'teriamos', 'tinhamos', 'tiveramos', 'tivessemos', 'voce', 'voces'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 314)\n",
            "Número de features finais: 314\n",
            "\n",
            "Algumas features seleccionadas:\n",
            "['abril', 'agua', 'aguas', 'ajuda', 'alcanca', 'alegria', 'alto', 'amanha', 'amigo', 'amigos', 'amizade', 'amor', 'amores', 'anda', 'andar', 'ano', 'antes', 'apanha', 'aprende', 'arvore', 'assim', 'ate', 'baixo', 'barriga', 'basta', 'bebe', 'beleza', 'bem', 'bem nao', 'boa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Exemplo 1.5 – Representação TF-IDF com TfidfVectorizer"
      ],
      "metadata": {
        "id": "4Kc6-uH_ZPwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "Cambiar el modelo de representación de texto:\n",
        "👉 pasar de contar palabras (Bag-of-Words) a ponderarlas según su importancia (TF-IDF).\n",
        "\n",
        "Esto mejora el rendimiento de muchos modelos, ya que no todas las palabras tienen el mismo peso en la comprensión de un texto.\n",
        "\n",
        "🧠 Explicación teórica\n",
        "\n",
        "El TF-IDF combina dos ideas:\n",
        "\n",
        "TF (Term Frequency) → mide cuántas veces aparece una palabra en un documento.\n",
        "\n",
        "Cuanto más aparece, más relevante es para ese documento.\n",
        "\n",
        "Ejemplo: si en un proverbio la palabra “vida” aparece 3 veces, TF(vida)=3.\n",
        "\n",
        "IDF (Inverse Document Frequency) → mide en cuántos documentos aparece esa palabra.\n",
        "\n",
        "Cuanto más común sea en todos los textos, menos informativa es.\n",
        "\n",
        "Ejemplo: si “vida” aparece en 90 % de los proverbios, su IDF será pequeño.\n",
        "\n",
        "💬 Interpretación\n",
        "\n",
        "A diferencia de CountVectorizer, aquí los valores no son enteros, sino ponderaciones decimales (entre 0 y 1).\n",
        "\n",
        "Palabras muy frecuentes en un documento, pero poco comunes en el corpus total, tienen un peso TF-IDF alto.\n",
        "\n",
        "Las palabras comunes (como “de”, “o”, “a”) tienen un peso bajo o nulo.\n",
        "\n",
        "📈 Así, la representación TF-IDF resalta las palabras más características de cada texto."
      ],
      "metadata": {
        "id": "BYfcYn9QrIy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Exemplo 1.5 – Representação TF-IDF com *TfidfVectorizer*\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Objetivo\n",
        "Cambiar el modelo de representación de texto:\n",
        "\n",
        "👉 Pasar de **contar palabras (Bag-of-Words)** a **ponderarlas según su importancia (TF-IDF)**.  \n",
        "\n",
        "Esto mejora el rendimiento de muchos modelos, ya que no todas las palabras tienen el mismo peso en la comprensión de un texto.\n",
        "\n",
        "---\n",
        "\n",
        "### 💡 Explicación teórica\n",
        "\n",
        "El **TF-IDF** combina dos ideas fundamentales:\n",
        "\n",
        "#### 🔸 TF (*Term Frequency*)\n",
        "Mide cuántas veces aparece una palabra en un documento.  \n",
        "Cuanto más aparece, más relevante es para ese documento.\n",
        "\n",
        "📘 Ejemplo:  \n",
        "Si en un proverbio la palabra *vida* aparece 3 veces,  \n",
        "entonces TF(vida) = 3.\n",
        "\n",
        "#### 🔸 IDF (*Inverse Document Frequency*)\n",
        "Mide en cuántos documentos aparece esa palabra.  \n",
        "Cuanto más común sea en todos los textos, menos informativa es.\n",
        "\n",
        "📘 Ejemplo:  \n",
        "Si *vida* aparece en 90 % de los proverbios, su IDF será pequeño.\n",
        "\n",
        "---\n",
        "\n",
        "### 📈 Interpretación\n",
        "\n",
        "A diferencia de **CountVectorizer**, aquí los valores no son enteros,  \n",
        "sino **ponderaciones decimales** (entre 0 y 1).  \n",
        "\n",
        "- Palabras muy frecuentes en un documento, pero poco comunes en el corpus total,  \n",
        "  → tienen un peso **TF-IDF alto**.  \n",
        "- Palabras comunes (como *de*, *o*, *a*)  \n",
        "  → tienen un peso **bajo o nulo**.  \n",
        "\n",
        "🧾 Así, la representación **TF-IDF resalta las palabras más características** de cada texto.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 Fórmula del cálculo TF-IDF\n",
        "\n",
        "El valor **TF-IDF** de un término $t$ en un documento $d$ dentro de un corpus de $N$ documentos se calcula como:\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\log \\left( \\frac{N}{\\text{DF}(t)} \\right)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $\\text{TF}(t, d)$ → frecuencia del término *t* en el documento *d*  \n",
        "- $\\text{DF}(t)$ → número de documentos que contienen el término *t*  \n",
        "- $N$ → número total de documentos del corpus"
      ],
      "metadata": {
        "id": "xUBR6aChtATF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador TF-IDF\n",
        "tf_vect = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),  # unigramas y bigramas\n",
        "    min_df=3,            # debe aparecer en al menos 3 documentos\n",
        "    max_df=0.5,          # y en no más del 50% de los documentos\n",
        "    max_features=500     # máximo de 500 features\n",
        ")\n",
        "\n",
        "# Generamos la matriz TF-IDF\n",
        "p_tf = tf_vect.fit_transform(docs)\n",
        "\n",
        "# Mostramos resultados básicos\n",
        "print(\"Forma da matriz:\", p_tf.shape)\n",
        "print(\"Número de features:\", len(tf_vect.get_feature_names_out()))\n",
        "\n",
        "# Mostramos algunas features y valores TF-IDF del primer documento\n",
        "print(\"\\nFeatures (palabras o n-gramas):\")\n",
        "print(list(tf_vect.get_feature_names_out())[:20])\n",
        "\n",
        "print(\"\\nRepresentação do primeiro documento (valores TF-IDF):\")\n",
        "print(p_tf[0, :].toarray().tolist())\n"
      ],
      "metadata": {
        "id": "dDC__DCntSY3",
        "outputId": "38d97ed9-4e5c-4116-ef51-d56bd825715e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 500)\n",
            "Número de features: 500\n",
            "\n",
            "Features (palabras o n-gramas):\n",
            "['abril', 'acaba', 'ainda', 'ajuda', 'alcança', 'alegria', 'alheia', 'alheio', 'alma', 'alto', 'ama', 'amanhã', 'ambição', 'amigo', 'amigos', 'amizade', 'amor', 'amores', 'anda', 'andar']\n",
            "\n",
            "Representação do primeiro documento (valores TF-IDF):\n",
            "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9382731425466352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34589522976713843, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Exemplo 1.6 – Reutilizar um Vectorizer com novos documentos"
      ],
      "metadata": {
        "id": "1ET0SEztZ9Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "Aprender a representar nuevos documentos (no vistos durante el entrenamiento) usando el mismo vectorizador ya entrenado.\n",
        "Esto es fundamental cuando trabajas con modelos de aprendizaje automático, ya que:\n",
        "\n",
        "⚠️ El vocabulario del modelo debe mantenerse constante.\n",
        "Si reentrenas el vectorizador con nuevos textos, el vocabulario puede cambiar,\n",
        "y los vectores dejarían de ser comparables.\n",
        "\n",
        "🧠 Explicación teórica\n",
        "\n",
        "Cuando entrenamos un CountVectorizer o TfidfVectorizer, ocurren dos pasos:\n",
        "\n",
        "Método\tQué hace\n",
        "fit_transform(docs)\tAprende el vocabulario del corpus y transforma los documentos en vectores. (Se usa con los datos de entrenamiento).\n",
        "transform(new_docs)\tUsa el vocabulario aprendido previamente para transformar nuevos documentos.\n",
        "\n",
        "💡 Es decir:\n",
        "\n",
        "fit_transform() se usa una sola vez durante el entrenamiento.\n",
        "\n",
        "transform() se usa cada vez que quieres representar nuevos textos con el mismo modelo"
      ],
      "metadata": {
        "id": "bPGPZhBXtk5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargamos los proverbios (entrenamiento)\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos y entrenamos el vectorizador\n",
        "tf_vect = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),   # unigramas y bigramas\n",
        "    min_df=3,             # debe aparecer en al menos 3 documentos\n",
        "    max_df=0.5,           # no más del 50% de los documentos\n",
        "    max_features=500      # máximo de 500 palabras o n-gramas\n",
        ")\n",
        "p_tf = tf_vect.fit_transform(docs)\n",
        "\n",
        "# Nuevos documentos (no vistos durante el entrenamiento)\n",
        "novos_docs = [\n",
        "    \"amor é fogo que arde sem se ver\",\n",
        "    \"verdes são os campos\"\n",
        "]\n",
        "\n",
        "# Transformamos los nuevos documentos con el mismo vectorizador\n",
        "novos_vects = tf_vect.transform(novos_docs)\n",
        "\n",
        "# Exploramos la matriz resultante\n",
        "print(\"Forma da matriz dos novos documentos:\", novos_vects.shape)\n",
        "print(\"\\nRepresentação TF-IDF dos novos documentos:\")\n",
        "print(novos_vects.toarray())\n",
        "\n",
        "\n",
        "#🧾 Explicación\n",
        "  # tf_vect.fit_transform(docs) → se usa solo con los proverbios iniciales (entrenamiento).\n",
        "  # tf_vect.transform(novos_docs) → aplica el mismo vocabulario al nuevo texto.\n",
        "  # Palabras que no estaban en el vocabulario original serán ignoradas.\n",
        "\n",
        "# Output es un array (2, 500) donde solo hay decimales en aquellas posiciones de las palbras del documento que marcan su importancia (de 0 a 1) y 0 en el resto, las palabras que no esten en el modelo del vocubulario\n",
        "# inicial entonces serán obviadas\n"
      ],
      "metadata": {
        "id": "XIh9nO6HuD3Q",
        "outputId": "34fe7262-e220-4148-9d43-e921917822d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz dos novos documentos: (2, 500)\n",
            "\n",
            "Representação TF-IDF dos novos documentos:\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.46215083 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.50949558\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.21335991 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.23497398 0.\n",
            "  0.         0.         0.         0.         0.         0.38080138\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.53017539 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.64998539 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.7599467\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 1.3 Desafio – Similaridade do Cosseno entre documentos"
      ],
      "metadata": {
        "id": "RhdQyMwlawWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "El desafío propone que crees una función en Python que, dada una colección de documentos y un nuevo documento, encuentre los n documentos más parecidos según la similaridad del coseno.\n",
        "\n",
        "Esto te permitirá medir qué tan similares son dos textos basándote en su representación numérica (por ejemplo, TF-IDF).\n",
        "\n",
        "🧠 Explicación teórica\n",
        "\n",
        "Cuando representamos los documentos con TF-IDF, cada documento se convierte en un vector de características numéricas.\n",
        "Para medir la semejanza entre dos documentos, usamos la similaridad del coseno (cosine similarity)."
      ],
      "metadata": {
        "id": "qojDSanKw65O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPCION 1 (CHATI)"
      ],
      "metadata": {
        "id": "vauYf5Luzniw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1️⃣ Cargar los documentos base\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# 2️⃣ Crear y entrenar el vectorizador TF-IDF\n",
        "tf_vect = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.5)\n",
        "tf_matrix = tf_vect.fit_transform(docs)\n",
        "\n",
        "# 3️⃣ Definir la función que devuelve los n documentos más similares\n",
        "def documentos_parecidos(tf_vect, tf_matrix, novo_doc, n=3):\n",
        "    \"\"\"\n",
        "    Dado un vectorizador TF-IDF entrenado, su matriz y un nuevo documento,\n",
        "    devuelve los n documentos más parecidos según la similaridad del coseno.\n",
        "    \"\"\"\n",
        "    # Transformar el nuevo documento al mismo espacio vectorial\n",
        "    novo_vec = tf_vect.transform([novo_doc])\n",
        "\n",
        "    # Calcular la similaridad del coseno con todos los documentos\n",
        "    similaridades = cosine_similarity(novo_vec, tf_matrix)[0]\n",
        "\n",
        "    # Obtener los índices de los documentos más similares\n",
        "    indices = np.argsort(similaridades)[::-1][:n]\n",
        "\n",
        "    # Devolver los documentos y sus puntuaciones\n",
        "    resultados = [(docs[i], similaridades[i]) for i in indices]\n",
        "    return resultados\n",
        "\n",
        "# 4️⃣ Probar la función con un nuevo texto\n",
        "novo_texto = \"quem espera sempre alcança\"\n",
        "resultados = documentos_parecidos(tf_vect, tf_matrix, novo_texto, n=5)\n",
        "\n",
        "print(\"🔎 Documentos mais semelhantes a:\", novo_texto)\n",
        "for doc, sim in resultados:\n",
        "    print(f\"{sim:.3f} → {doc}\")\n"
      ],
      "metadata": {
        "id": "QJ0HPoyha02y",
        "outputId": "64f010d2-0f92-4f9c-97b1-01ef271cc113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Documentos mais semelhantes a: quem espera sempre alcança\n",
            "1.000 → quem espera sempre alcança.\n",
            "0.643 → quem porfia sempre alcança.\n",
            "0.577 → quem tem esperança sempre alcança.\n",
            "0.547 → quem espera desespera.\n",
            "0.420 → quem espera por sapatos de defunto, toda a vida anda descalço.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPCION 2 (CLASE)"
      ],
      "metadata": {
        "id": "0v4tYgn5zqTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar(docs, doc, n=10, smin=0.5):\n",
        "    tf_vect = TfidfVectorizer(ngram_range=(1,3), strip_accents='unicode',\n",
        "                              max_features=500, min_df=5, max_df=0.75)\n",
        "\n",
        "    vects = tf_vect.fit_transform(docs)\n",
        "    doc_vec = tf_vect.transform([doc])\n",
        "\n",
        "    vocabulary = tf_vect.get_feature_names_out()\n",
        "    for i in doc_vec.indices:\n",
        "        print(i, vocabulary[i])\n",
        "\n",
        "    # cálculo da similaridade\n",
        "    cosine_matrix = cosine_similarity(vects, doc_vec)\n",
        "    print(cosine_matrix.shape)     # Genera una matriz de tamaño (n_docs, 1) con la similitud entre cada documento del corpus y el nuevo documento.\n",
        "\n",
        "    lista_cos = [(docs[i], e[0]) for i, e in enumerate(cosine_matrix)]\n",
        "    lista_cos.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [(d, c) for d, c in lista_cos[:n] if c > smin]\n",
        "\n",
        "doc = 'Em terra de cegos, quem tem um olho é rei.'\n",
        "similar = most_similar(docs, doc)\n",
        "print(similar)"
      ],
      "metadata": {
        "id": "zcz8xjXIzs90",
        "outputId": "19028459-e0b6-497f-fd9d-fa06cbe87953",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85 de\n",
            "114 em\n",
            "116 em terra\n",
            "262 olho\n",
            "319 quem\n",
            "331 quem tem\n",
            "337 rei\n",
            "383 tem\n",
            "390 terra\n",
            "391 terra de\n",
            "406 um\n",
            "(1617, 1)\n",
            "[('em terra de cegos, quem tem um olho é rei.', np.float64(1.0)), ('em terra de sapos, de cócoras como eles.', np.float64(0.6338665651998857)), ('em terra de saci, calça comprida dá pra dois.', np.float64(0.6058372801802083)), ('em terra de sapo, mosquito não dá rasante.', np.float64(0.5772407093557776))]\n"
          ]
        }
      ]
    }
  ]
}