{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMp+cOIJmJfElyP6fATakGq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franfgv9/PLN/blob/main/Practica_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5KhW5wcJ9CL",
        "outputId": "65aa4af9-1c7a-4fdb-c369-a48298e86160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-05 22:53:51--  https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/proverbios_N.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68195 (67K) [text/plain]\n",
            "Saving to: ‚Äòproverbios.txt‚Äô\n",
            "\n",
            "proverbios.txt      100%[===================>]  66.60K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-10-05 22:53:51 (3.51 MB/s) - ‚Äòproverbios.txt‚Äô saved [68195/68195]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/proverbios_N.txt -O proverbios.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "El objetivo inicial es crear o cargar una colecci√≥n de documentos de texto y representarlos como una lista de cadenas de texto en Python.\n",
        "Cada l√≠nea de texto del fichero representar√° un documento (por ejemplo, un proverbio o una frase).\n",
        "\n",
        "üß† Explicaci√≥n te√≥rica\n",
        "\n",
        "El primer paso en cualquier proyecto de procesamiento de lenguaje natural es obtener los datos textuales."
      ],
      "metadata": {
        "id": "-3fLPnfUkLQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentario = '#'\n",
        "def carrega_docs ( ficheiro ) :\n",
        "  docs = []\n",
        "  with open ( ficheiro , encoding ='utf -8 ') as f :\n",
        "    lines = f . readlines ()\n",
        "    for txt in lines :\n",
        "      txt = txt . strip ()\n",
        "      if len ( txt ) > 0 and txt [0] != comentario :\n",
        "        docs . append ( txt )\n",
        "  return docs\n",
        "\n",
        "# Se abre el fichero (open(ficheiro, encoding='utf-8'))\n",
        "# Se leen todas las l√≠neas (readlines())\n",
        "# Se limpian los espacios y saltos de l√≠nea (strip())\n",
        "# Se ignoran comentarios o l√≠neas vac√≠as\n",
        "# Finalmente, se devuelve una lista de documentos, donde cada elemento es una cadena de texto (un documento independiente).\n"
      ],
      "metadata": {
        "id": "7GtaW4QFU6Vr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Exemplo 1.2 ‚Äì Bag of Words (Bolsa de Palavras)"
      ],
      "metadata": {
        "id": "5Zz5lyzNW-PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "Transformar una colecci√≥n de textos en una matriz num√©rica que indique cu√°ntas veces aparece cada palabra en cada documento.\n",
        "Esta representaci√≥n es fundamental porque los algoritmos de Machine Learning no pueden trabajar directamente con texto: necesitan n√∫meros.\n",
        "\n",
        "\n",
        "üß† Explicaci√≥n te√≥rica\n",
        "\n",
        "El modelo Bag-of-Words (BoW) consiste en:\n",
        "\n",
        "  1. Crear un vocabulario con todas las palabras √∫nicas del corpus.\n",
        "\n",
        "  2. Representar cada documento como un vector de frecuencias de esas palabras.\n",
        "\n",
        "üìä Cada fila = un documento\n",
        "üìà Cada columna = una palabra del vocabulario\n",
        "üíæ Cada valor = n√∫mero de veces que la palabra aparece en el documento"
      ],
      "metadata": {
        "id": "pPyhq46-ksdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Cargamos los documentos (por ejemplo, los proverbios)\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador\n",
        "c_vect = CountVectorizer()\n",
        "\n",
        "# Transformamos los documentos en una matriz documento √ó t√©rmino\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Visualizamos la forma de la matriz (n¬∫ de documentos, n¬∫ de palabras)\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "\n",
        "# Obtenemos el vocabulario (todas las palabras √∫nicas)\n",
        "print(\"Vocabulario:\", c_vect.get_feature_names_out())\n",
        "\n",
        "# Visualizamos el vector del primer documento\n",
        "print(\"Representa√ß√£o do primeiro documento:\")\n",
        "print(p_counts[0, :].toarray().tolist())\n",
        "print(p_counts[0, :])\n",
        "print(docs[0])\n"
      ],
      "metadata": {
        "id": "tD6JqH3JluCL",
        "outputId": "92e09faf-a4ce-4cd1-da0b-a04562ee1578",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 2741)\n",
            "Vocabulario: ['abafadi√ßo' 'abaixa' 'abaixo' ... '√∫ltimo' '√∫ltimos' '√∫til']\n",
            "Representa√ß√£o do primeiro documento:\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 4 stored elements and shape (1, 2741)>\n",
            "  Coords\tValues\n",
            "  (0, 7)\t1\n",
            "  (0, 1767)\t1\n",
            "  (0, 1430)\t1\n",
            "  (0, 546)\t1\n",
            "a abelha n√£o leva chumbo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Exemplo 1.3 ‚Äì N-gramas com CountVectorizer"
      ],
      "metadata": {
        "id": "4rrCX0Z3XABF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "Mejorar la representaci√≥n del texto considerando no solo palabras individuales, sino tambi√©n secuencias de palabras consecutivas (los llamados n-gramas).\n",
        "Esto permite capturar expresiones y contextos que el modelo Bag-of-Words simple no detecta.\n",
        "\n",
        "üîç ¬øPor qu√© usar n-gramas?\n",
        "\n",
        "Unigramas capturan las palabras por separado (√∫tiles para vocabularios amplios\n",
        "\n",
        "Bigramas o trigramas capturan contexto local:\n",
        "\n",
        "  ‚Äún√£o gosto‚Äù ‚â† ‚Äúgosto‚Äù\n",
        "  \n",
        "  ‚Äúmuito bom‚Äù ‚Üí sentimiento positivo que una sola palabra no revela."
      ],
      "metadata": {
        "id": "yNajtNcdnzFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador con n-gramas (de 1 a 3 palabras)\n",
        "c_vect = CountVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "# Representamos los documentos en una matriz documento √ó t√©rmino\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Mostramos informaci√≥n sobre la matriz generada\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "print(\"N√∫mero de documentos:\", p_counts.shape[0])\n",
        "print(\"Tamanho do vocabul√°rio (features):\", p_counts.shape[1])\n",
        "\n",
        "# Mostramos algunas features detectadas\n",
        "print(\"\\nExemplos de features (n-gramas detectados):\")\n",
        "print(list(c_vect.get_feature_names_out())[:30])  # solo los 30 primeros\n",
        "\n",
        "# COMENTARIO: logicamente ahora vemos mas vocabulario que antes porque ahora al tener n-gramas tenemos mas combinaicones de palabras nuevas"
      ],
      "metadata": {
        "id": "FPJhAOWFoYEU",
        "outputId": "c8cd8492-c321-49bd-fcb3-93f2a8a2c836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 17120)\n",
            "N√∫mero de documentos: 1617\n",
            "Tamanho do vocabul√°rio (features): 17120\n",
            "\n",
            "Exemplos de features (n-gramas detectados):\n",
            "['abafadi√ßo', 'abafadi√ßo sai', 'abafadi√ßo sai abelha', 'abaixa', 'abaixa rabo', 'abaixa rabo se', 'abaixo', 'abaixo come', 'abandona', 'abarca', 'abarca pouco', 'abarca pouco abra√ßa', 'abastado', 'abastado quem', 'abastado quem d√°', 'abeg√£o', 'abelha', 'abelha do', 'abelha do corti√ßo', 'abelha n√£o', 'abelha n√£o leva', 'abelha sen√£o', 'abelha sen√£o quem', 'abelhas', 'abelhas para', 'abelhas para as', 'aben√ßoada', 'aberta', 'aberta ou', 'aberta ou entra']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Exemplo 1.4 ‚Äì Ajustes avan√ßados em CountVectorizer"
      ],
      "metadata": {
        "id": "fj2-yRnYY2Pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_features:\tDefine un n√∫mero m√°ximo de palabras a conservar. Usa las m√°s frecuentes.\t  max_features=1000 mantiene solo las 1000 palabras m√°s comunes.\n",
        "\n",
        "max_df:\tPalabras que aparecen en demasiados documentos se descartan. Puede ser valor absoluto o proporci√≥n.\t max_df=0.75 ignora palabras que aparecen en ‚â•75 % de los documentos.\n",
        "\n",
        "min_df:\tPalabras que aparecen muy poco tambi√©n se descartan.\tmin_df=5 mantiene solo palabras que aparecen en ‚â•5 documentos.\n",
        "\n",
        "strip_accents:\tElimina acentos antes del an√°lisis.\t\"cora√ß√£o\" ‚Üí \"coracao\"\n",
        "\n",
        "stop_words:\tPermite eliminar stopwords (palabras vac√≠as sin significado, como ‚Äúo‚Äù, ‚Äúde‚Äù, ‚Äúem‚Äù).\tSe puede usar la lista de stopwords del idioma portugu√©s en NLTK."
      ],
      "metadata": {
        "id": "Au7ZeZNupkRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "\n",
        "# Descargamos las stopwords de NLTK (solo la primera vez)\n",
        "nltk.download('stopwords')\n",
        "pt_stop = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador con par√°metros personalizados\n",
        "c_vect = CountVectorizer(\n",
        "    ngram_range=(1, 3),       # unigramas, bigramas y trigramas\n",
        "    strip_accents='unicode',  # elimina acentos\n",
        "    stop_words=pt_stop,       # usa stopwords en portugu√©s\n",
        "    min_df=5,                 # ignora palabras que aparecen en <5 documentos\n",
        "    max_df=0.75,              # ignora palabras que aparecen en >75% de documentos\n",
        "    max_features=1000         # mantiene las 1000 m√°s frecuentes\n",
        ")\n",
        "\n",
        "# Generamos la matriz documento √ó t√©rmino\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Informaci√≥n sobre la matriz\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "print(\"N√∫mero de features finais:\", len(c_vect.get_feature_names_out()))\n",
        "\n",
        "# Visualizamos algunas features seleccionadas\n",
        "print(\"\\nAlgumas features seleccionadas:\")\n",
        "print(list(c_vect.get_feature_names_out())[:30])\n"
      ],
      "metadata": {
        "id": "K5-IldQiqc_w",
        "outputId": "7b20ccd0-a2be-44c3-f3c0-e045b06fdc6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ate', 'eramos', 'estao', 'estavamos', 'estiveramos', 'estivessemos', 'foramos', 'fossemos', 'ha', 'hao', 'houveramos', 'houverao', 'houveriamos', 'houvessemos', 'ja', 'nao', 'sao', 'sera', 'serao', 'seriamos', 'so', 'tambem', 'tera', 'terao', 'teriamos', 'tinhamos', 'tiveramos', 'tivessemos', 'voce', 'voces'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 314)\n",
            "N√∫mero de features finais: 314\n",
            "\n",
            "Algumas features seleccionadas:\n",
            "['abril', 'agua', 'aguas', 'ajuda', 'alcanca', 'alegria', 'alto', 'amanha', 'amigo', 'amigos', 'amizade', 'amor', 'amores', 'anda', 'andar', 'ano', 'antes', 'apanha', 'aprende', 'arvore', 'assim', 'ate', 'baixo', 'barriga', 'basta', 'bebe', 'beleza', 'bem', 'bem nao', 'boa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Exemplo 1.5 ‚Äì Representa√ß√£o TF-IDF com TfidfVectorizer"
      ],
      "metadata": {
        "id": "4Kc6-uH_ZPwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "Cambiar el modelo de representaci√≥n de texto:\n",
        "üëâ pasar de contar palabras (Bag-of-Words) a ponderarlas seg√∫n su importancia (TF-IDF).\n",
        "\n",
        "Esto mejora el rendimiento de muchos modelos, ya que no todas las palabras tienen el mismo peso en la comprensi√≥n de un texto.\n",
        "\n",
        "üß† Explicaci√≥n te√≥rica\n",
        "\n",
        "El TF-IDF combina dos ideas:\n",
        "\n",
        "TF (Term Frequency) ‚Üí mide cu√°ntas veces aparece una palabra en un documento.\n",
        "\n",
        "Cuanto m√°s aparece, m√°s relevante es para ese documento.\n",
        "\n",
        "Ejemplo: si en un proverbio la palabra ‚Äúvida‚Äù aparece 3 veces, TF(vida)=3.\n",
        "\n",
        "IDF (Inverse Document Frequency) ‚Üí mide en cu√°ntos documentos aparece esa palabra.\n",
        "\n",
        "Cuanto m√°s com√∫n sea en todos los textos, menos informativa es.\n",
        "\n",
        "Ejemplo: si ‚Äúvida‚Äù aparece en 90 % de los proverbios, su IDF ser√° peque√±o.\n",
        "\n",
        "üí¨ Interpretaci√≥n\n",
        "\n",
        "A diferencia de CountVectorizer, aqu√≠ los valores no son enteros, sino ponderaciones decimales (entre 0 y 1).\n",
        "\n",
        "Palabras muy frecuentes en un documento, pero poco comunes en el corpus total, tienen un peso TF-IDF alto.\n",
        "\n",
        "Las palabras comunes (como ‚Äúde‚Äù, ‚Äúo‚Äù, ‚Äúa‚Äù) tienen un peso bajo o nulo.\n",
        "\n",
        "üìà As√≠, la representaci√≥n TF-IDF resalta las palabras m√°s caracter√≠sticas de cada texto."
      ],
      "metadata": {
        "id": "BYfcYn9QrIy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Exemplo 1.5 ‚Äì Representa√ß√£o TF-IDF com *TfidfVectorizer*\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Objetivo\n",
        "Cambiar el modelo de representaci√≥n de texto:\n",
        "\n",
        "üëâ Pasar de **contar palabras (Bag-of-Words)** a **ponderarlas seg√∫n su importancia (TF-IDF)**.  \n",
        "\n",
        "Esto mejora el rendimiento de muchos modelos, ya que no todas las palabras tienen el mismo peso en la comprensi√≥n de un texto.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Explicaci√≥n te√≥rica\n",
        "\n",
        "El **TF-IDF** combina dos ideas fundamentales:\n",
        "\n",
        "#### üî∏ TF (*Term Frequency*)\n",
        "Mide cu√°ntas veces aparece una palabra en un documento.  \n",
        "Cuanto m√°s aparece, m√°s relevante es para ese documento.\n",
        "\n",
        "üìò Ejemplo:  \n",
        "Si en un proverbio la palabra *vida* aparece 3 veces,  \n",
        "entonces TF(vida) = 3.\n",
        "\n",
        "#### üî∏ IDF (*Inverse Document Frequency*)\n",
        "Mide en cu√°ntos documentos aparece esa palabra.  \n",
        "Cuanto m√°s com√∫n sea en todos los textos, menos informativa es.\n",
        "\n",
        "üìò Ejemplo:  \n",
        "Si *vida* aparece en 90 % de los proverbios, su IDF ser√° peque√±o.\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Interpretaci√≥n\n",
        "\n",
        "A diferencia de **CountVectorizer**, aqu√≠ los valores no son enteros,  \n",
        "sino **ponderaciones decimales** (entre 0 y 1).  \n",
        "\n",
        "- Palabras muy frecuentes en un documento, pero poco comunes en el corpus total,  \n",
        "  ‚Üí tienen un peso **TF-IDF alto**.  \n",
        "- Palabras comunes (como *de*, *o*, *a*)  \n",
        "  ‚Üí tienen un peso **bajo o nulo**.  \n",
        "\n",
        "üßæ As√≠, la representaci√≥n **TF-IDF resalta las palabras m√°s caracter√≠sticas** de cada texto.\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ F√≥rmula del c√°lculo TF-IDF\n",
        "\n",
        "El valor **TF-IDF** de un t√©rmino $t$ en un documento $d$ dentro de un corpus de $N$ documentos se calcula como:\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\log \\left( \\frac{N}{\\text{DF}(t)} \\right)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $\\text{TF}(t, d)$ ‚Üí frecuencia del t√©rmino *t* en el documento *d*  \n",
        "- $\\text{DF}(t)$ ‚Üí n√∫mero de documentos que contienen el t√©rmino *t*  \n",
        "- $N$ ‚Üí n√∫mero total de documentos del corpus"
      ],
      "metadata": {
        "id": "xUBR6aChtATF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador TF-IDF\n",
        "tf_vect = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),  # unigramas y bigramas\n",
        "    min_df=3,            # debe aparecer en al menos 3 documentos\n",
        "    max_df=0.5,          # y en no m√°s del 50% de los documentos\n",
        "    max_features=500     # m√°ximo de 500 features\n",
        ")\n",
        "\n",
        "# Generamos la matriz TF-IDF\n",
        "p_tf = tf_vect.fit_transform(docs)\n",
        "\n",
        "# Mostramos resultados b√°sicos\n",
        "print(\"Forma da matriz:\", p_tf.shape)\n",
        "print(\"N√∫mero de features:\", len(tf_vect.get_feature_names_out()))\n",
        "\n",
        "# Mostramos algunas features y valores TF-IDF del primer documento\n",
        "print(\"\\nFeatures (palabras o n-gramas):\")\n",
        "print(list(tf_vect.get_feature_names_out())[:20])\n",
        "\n",
        "print(\"\\nRepresenta√ß√£o do primeiro documento (valores TF-IDF):\")\n",
        "print(p_tf[0, :].toarray().tolist())\n"
      ],
      "metadata": {
        "id": "dDC__DCntSY3",
        "outputId": "38d97ed9-4e5c-4116-ef51-d56bd825715e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 500)\n",
            "N√∫mero de features: 500\n",
            "\n",
            "Features (palabras o n-gramas):\n",
            "['abril', 'acaba', 'ainda', 'ajuda', 'alcan√ßa', 'alegria', 'alheia', 'alheio', 'alma', 'alto', 'ama', 'amanh√£', 'ambi√ß√£o', 'amigo', 'amigos', 'amizade', 'amor', 'amores', 'anda', 'andar']\n",
            "\n",
            "Representa√ß√£o do primeiro documento (valores TF-IDF):\n",
            "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9382731425466352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34589522976713843, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Exemplo 1.6 ‚Äì Reutilizar um Vectorizer com novos documentos"
      ],
      "metadata": {
        "id": "1ET0SEztZ9Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "Aprender a representar nuevos documentos (no vistos durante el entrenamiento) usando el mismo vectorizador ya entrenado.\n",
        "Esto es fundamental cuando trabajas con modelos de aprendizaje autom√°tico, ya que:\n",
        "\n",
        "‚ö†Ô∏è El vocabulario del modelo debe mantenerse constante.\n",
        "Si reentrenas el vectorizador con nuevos textos, el vocabulario puede cambiar,\n",
        "y los vectores dejar√≠an de ser comparables.\n",
        "\n",
        "üß† Explicaci√≥n te√≥rica\n",
        "\n",
        "Cuando entrenamos un CountVectorizer o TfidfVectorizer, ocurren dos pasos:\n",
        "\n",
        "M√©todo\tQu√© hace\n",
        "fit_transform(docs)\tAprende el vocabulario del corpus y transforma los documentos en vectores. (Se usa con los datos de entrenamiento).\n",
        "transform(new_docs)\tUsa el vocabulario aprendido previamente para transformar nuevos documentos.\n",
        "\n",
        "üí° Es decir:\n",
        "\n",
        "fit_transform() se usa una sola vez durante el entrenamiento.\n",
        "\n",
        "transform() se usa cada vez que quieres representar nuevos textos con el mismo modelo"
      ],
      "metadata": {
        "id": "bPGPZhBXtk5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargamos los proverbios (entrenamiento)\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos y entrenamos el vectorizador\n",
        "tf_vect = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),   # unigramas y bigramas\n",
        "    min_df=3,             # debe aparecer en al menos 3 documentos\n",
        "    max_df=0.5,           # no m√°s del 50% de los documentos\n",
        "    max_features=500      # m√°ximo de 500 palabras o n-gramas\n",
        ")\n",
        "p_tf = tf_vect.fit_transform(docs)\n",
        "\n",
        "# Nuevos documentos (no vistos durante el entrenamiento)\n",
        "novos_docs = [\n",
        "    \"amor √© fogo que arde sem se ver\",\n",
        "    \"verdes s√£o os campos\"\n",
        "]\n",
        "\n",
        "# Transformamos los nuevos documentos con el mismo vectorizador\n",
        "novos_vects = tf_vect.transform(novos_docs)\n",
        "\n",
        "# Exploramos la matriz resultante\n",
        "print(\"Forma da matriz dos novos documentos:\", novos_vects.shape)\n",
        "print(\"\\nRepresenta√ß√£o TF-IDF dos novos documentos:\")\n",
        "print(novos_vects.toarray())\n",
        "\n",
        "\n",
        "#üßæ Explicaci√≥n\n",
        "  # tf_vect.fit_transform(docs) ‚Üí se usa solo con los proverbios iniciales (entrenamiento).\n",
        "  # tf_vect.transform(novos_docs) ‚Üí aplica el mismo vocabulario al nuevo texto.\n",
        "  # Palabras que no estaban en el vocabulario original ser√°n ignoradas.\n",
        "\n",
        "# Output es un array (2, 500) donde solo hay decimales en aquellas posiciones de las palbras del documento que marcan su importancia (de 0 a 1) y 0 en el resto, las palabras que no esten en el modelo del vocubulario\n",
        "# inicial entonces ser√°n obviadas\n"
      ],
      "metadata": {
        "id": "XIh9nO6HuD3Q",
        "outputId": "34fe7262-e220-4148-9d43-e921917822d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz dos novos documentos: (2, 500)\n",
            "\n",
            "Representa√ß√£o TF-IDF dos novos documentos:\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.46215083 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.50949558\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.21335991 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.23497398 0.\n",
            "  0.         0.         0.         0.         0.         0.38080138\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.53017539 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.64998539 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.7599467\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 1.3 Desafio ‚Äì Similaridade do Cosseno entre documentos"
      ],
      "metadata": {
        "id": "RhdQyMwlawWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "El desaf√≠o propone que crees una funci√≥n en Python que, dada una colecci√≥n de documentos y un nuevo documento, encuentre los n documentos m√°s parecidos seg√∫n la similaridad del coseno.\n",
        "\n",
        "Esto te permitir√° medir qu√© tan similares son dos textos bas√°ndote en su representaci√≥n num√©rica (por ejemplo, TF-IDF).\n",
        "\n",
        "üß† Explicaci√≥n te√≥rica\n",
        "\n",
        "Cuando representamos los documentos con TF-IDF, cada documento se convierte en un vector de caracter√≠sticas num√©ricas.\n",
        "Para medir la semejanza entre dos documentos, usamos la similaridad del coseno (cosine similarity)."
      ],
      "metadata": {
        "id": "qojDSanKw65O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPCION 1 (CHATI)"
      ],
      "metadata": {
        "id": "vauYf5Luzniw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1Ô∏è‚É£ Cargar los documentos base\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# 2Ô∏è‚É£ Crear y entrenar el vectorizador TF-IDF\n",
        "tf_vect = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.5)\n",
        "tf_matrix = tf_vect.fit_transform(docs)\n",
        "\n",
        "# 3Ô∏è‚É£ Definir la funci√≥n que devuelve los n documentos m√°s similares\n",
        "def documentos_parecidos(tf_vect, tf_matrix, novo_doc, n=3):\n",
        "    \"\"\"\n",
        "    Dado un vectorizador TF-IDF entrenado, su matriz y un nuevo documento,\n",
        "    devuelve los n documentos m√°s parecidos seg√∫n la similaridad del coseno.\n",
        "    \"\"\"\n",
        "    # Transformar el nuevo documento al mismo espacio vectorial\n",
        "    novo_vec = tf_vect.transform([novo_doc])\n",
        "\n",
        "    # Calcular la similaridad del coseno con todos los documentos\n",
        "    similaridades = cosine_similarity(novo_vec, tf_matrix)[0]\n",
        "\n",
        "    # Obtener los √≠ndices de los documentos m√°s similares\n",
        "    indices = np.argsort(similaridades)[::-1][:n]\n",
        "\n",
        "    # Devolver los documentos y sus puntuaciones\n",
        "    resultados = [(docs[i], similaridades[i]) for i in indices]\n",
        "    return resultados\n",
        "\n",
        "# 4Ô∏è‚É£ Probar la funci√≥n con un nuevo texto\n",
        "novo_texto = \"quem espera sempre alcan√ßa\"\n",
        "resultados = documentos_parecidos(tf_vect, tf_matrix, novo_texto, n=5)\n",
        "\n",
        "print(\"üîé Documentos mais semelhantes a:\", novo_texto)\n",
        "for doc, sim in resultados:\n",
        "    print(f\"{sim:.3f} ‚Üí {doc}\")\n"
      ],
      "metadata": {
        "id": "QJ0HPoyha02y",
        "outputId": "64f010d2-0f92-4f9c-97b1-01ef271cc113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Documentos mais semelhantes a: quem espera sempre alcan√ßa\n",
            "1.000 ‚Üí quem espera sempre alcan√ßa.\n",
            "0.643 ‚Üí quem porfia sempre alcan√ßa.\n",
            "0.577 ‚Üí quem tem esperan√ßa sempre alcan√ßa.\n",
            "0.547 ‚Üí quem espera desespera.\n",
            "0.420 ‚Üí quem espera por sapatos de defunto, toda a vida anda descal√ßo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPCION 2 (CLASE)"
      ],
      "metadata": {
        "id": "0v4tYgn5zqTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar(docs, doc, n=10, smin=0.5):\n",
        "    tf_vect = TfidfVectorizer(ngram_range=(1,3), strip_accents='unicode',\n",
        "                              max_features=500, min_df=5, max_df=0.75)\n",
        "\n",
        "    vects = tf_vect.fit_transform(docs)\n",
        "    doc_vec = tf_vect.transform([doc])\n",
        "\n",
        "    vocabulary = tf_vect.get_feature_names_out()\n",
        "    for i in doc_vec.indices:\n",
        "        print(i, vocabulary[i])\n",
        "\n",
        "    # c√°lculo da similaridade\n",
        "    cosine_matrix = cosine_similarity(vects, doc_vec)\n",
        "    print(cosine_matrix.shape)     # Genera una matriz de tama√±o (n_docs, 1) con la similitud entre cada documento del corpus y el nuevo documento.\n",
        "\n",
        "    lista_cos = [(docs[i], e[0]) for i, e in enumerate(cosine_matrix)]\n",
        "    lista_cos.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [(d, c) for d, c in lista_cos[:n] if c > smin]\n",
        "\n",
        "doc = 'Em terra de cegos, quem tem um olho √© rei.'\n",
        "similar = most_similar(docs, doc)\n",
        "print(similar)"
      ],
      "metadata": {
        "id": "zcz8xjXIzs90",
        "outputId": "19028459-e0b6-497f-fd9d-fa06cbe87953",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85 de\n",
            "114 em\n",
            "116 em terra\n",
            "262 olho\n",
            "319 quem\n",
            "331 quem tem\n",
            "337 rei\n",
            "383 tem\n",
            "390 terra\n",
            "391 terra de\n",
            "406 um\n",
            "(1617, 1)\n",
            "[('em terra de cegos, quem tem um olho √© rei.', np.float64(1.0)), ('em terra de sapos, de c√≥coras como eles.', np.float64(0.6338665651998857)), ('em terra de saci, cal√ßa comprida d√° pra dois.', np.float64(0.6058372801802083)), ('em terra de sapo, mosquito n√£o d√° rasante.', np.float64(0.5772407093557776))]\n"
          ]
        }
      ]
    }
  ]
}