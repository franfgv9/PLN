{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2grv0CXSR1blu2xFhx5Gh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franfgv9/PLN/blob/main/Practica_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5KhW5wcJ9CL",
        "outputId": "555aa56d-aa12-4aa3-f44e-a7211dd4159f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-07 10:25:35--  https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/proverbios_N.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68195 (67K) [text/plain]\n",
            "Saving to: ‚Äòproverbios.txt‚Äô\n",
            "\n",
            "\rproverbios.txt        0%[                    ]       0  --.-KB/s               \rproverbios.txt      100%[===================>]  66.60K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-10-07 10:25:35 (34.3 MB/s) - ‚Äòproverbios.txt‚Äô saved [68195/68195]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/proverbios_N.txt -O proverbios.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "El objetivo inicial es crear o cargar una colecci√≥n de documentos de texto y representarlos como una lista de cadenas de texto en Python.\n",
        "Cada l√≠nea de texto del fichero representar√° un documento (por ejemplo, un proverbio o una frase).\n",
        "\n",
        "üß† Explicaci√≥n te√≥rica\n",
        "\n",
        "El primer paso en cualquier proyecto de procesamiento de lenguaje natural es obtener los datos textuales."
      ],
      "metadata": {
        "id": "-3fLPnfUkLQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentario = '#'\n",
        "def carrega_docs ( ficheiro ) :\n",
        "  docs = []\n",
        "  with open ( ficheiro , encoding ='utf -8 ') as f :\n",
        "    lines = f . readlines ()\n",
        "    for txt in lines :\n",
        "      txt = txt . strip ()\n",
        "      if len ( txt ) > 0 and txt [0] != comentario :\n",
        "        docs . append ( txt )\n",
        "  return docs\n",
        "\n",
        "# Se abre el fichero (open(ficheiro, encoding='utf-8'))\n",
        "# Se leen todas las l√≠neas (readlines())\n",
        "# Se limpian los espacios y saltos de l√≠nea (strip())\n",
        "# Se ignoran comentarios o l√≠neas vac√≠as\n",
        "# Finalmente, se devuelve una lista de documentos, donde cada elemento es una cadena de texto (un documento independiente).\n"
      ],
      "metadata": {
        "id": "7GtaW4QFU6Vr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Exemplo 1.2 ‚Äì Bag of Words (Bolsa de Palavras)"
      ],
      "metadata": {
        "id": "5Zz5lyzNW-PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "Transformar una colecci√≥n de textos en una matriz num√©rica que indique cu√°ntas veces aparece cada palabra en cada documento.\n",
        "Esta representaci√≥n es fundamental porque los algoritmos de Machine Learning no pueden trabajar directamente con texto: necesitan n√∫meros.\n",
        "\n",
        "\n",
        "üß† Explicaci√≥n te√≥rica\n",
        "\n",
        "El modelo Bag-of-Words (BoW) consiste en:\n",
        "\n",
        "  1. Crear un vocabulario con todas las palabras √∫nicas del corpus.\n",
        "\n",
        "  2. Representar cada documento como un vector de frecuencias de esas palabras.\n",
        "\n",
        "üìä Cada fila = un documento\n",
        "üìà Cada columna = una palabra del vocabulario\n",
        "üíæ Cada valor = n√∫mero de veces que la palabra aparece en el documento"
      ],
      "metadata": {
        "id": "pPyhq46-ksdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Cargamos los documentos (por ejemplo, los proverbios)\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador\n",
        "c_vect = CountVectorizer()\n",
        "\n",
        "# Transformamos los documentos en una matriz documento √ó t√©rmino\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Visualizamos la forma de la matriz (n¬∫ de documentos, n¬∫ de palabras)\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "\n",
        "# Obtenemos el vocabulario (todas las palabras √∫nicas)\n",
        "print(\"Vocabulario:\", c_vect.get_feature_names_out())\n",
        "\n",
        "# Visualizamos el vector del primer documento\n",
        "print(\"Representa√ß√£o do primeiro documento:\")\n",
        "print(p_counts[0, :].toarray().tolist())\n",
        "print(p_counts[0, :])\n",
        "print(docs[0])\n"
      ],
      "metadata": {
        "id": "tD6JqH3JluCL",
        "outputId": "a5e16c91-098e-439a-dcc3-7dc8cafe95ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 2741)\n",
            "Vocabulario: ['abafadi√ßo' 'abaixa' 'abaixo' ... '√∫ltimo' '√∫ltimos' '√∫til']\n",
            "Representa√ß√£o do primeiro documento:\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 4 stored elements and shape (1, 2741)>\n",
            "  Coords\tValues\n",
            "  (0, 7)\t1\n",
            "  (0, 1767)\t1\n",
            "  (0, 1430)\t1\n",
            "  (0, 546)\t1\n",
            "a abelha n√£o leva chumbo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Exemplo 1.3 ‚Äì N-gramas com CountVectorizer"
      ],
      "metadata": {
        "id": "4rrCX0Z3XABF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "Mejorar la representaci√≥n del texto considerando no solo palabras individuales, sino tambi√©n secuencias de palabras consecutivas (los llamados n-gramas).\n",
        "Esto permite capturar expresiones y contextos que el modelo Bag-of-Words simple no detecta.\n",
        "\n",
        "üîç ¬øPor qu√© usar n-gramas?\n",
        "\n",
        "Unigramas capturan las palabras por separado (√∫tiles para vocabularios amplios\n",
        "\n",
        "Bigramas o trigramas capturan contexto local:\n",
        "\n",
        "  ‚Äún√£o gosto‚Äù ‚â† ‚Äúgosto‚Äù\n",
        "  \n",
        "  ‚Äúmuito bom‚Äù ‚Üí sentimiento positivo que una sola palabra no revela."
      ],
      "metadata": {
        "id": "yNajtNcdnzFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador con n-gramas (de 1 a 3 palabras)\n",
        "c_vect = CountVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "# Representamos los documentos en una matriz documento √ó t√©rmino\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Mostramos informaci√≥n sobre la matriz generada\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "print(\"N√∫mero de documentos:\", p_counts.shape[0])\n",
        "print(\"Tamanho do vocabul√°rio (features):\", p_counts.shape[1])\n",
        "\n",
        "# Mostramos algunas features detectadas\n",
        "print(\"\\nExemplos de features (n-gramas detectados):\")\n",
        "print(list(c_vect.get_feature_names_out())[:30])  # solo los 30 primeros\n",
        "\n",
        "# COMENTARIO: logicamente ahora vemos mas vocabulario que antes porque ahora al tener n-gramas tenemos mas combinaicones de palabras nuevas"
      ],
      "metadata": {
        "id": "FPJhAOWFoYEU",
        "outputId": "0be32ba0-7dec-45b8-e1df-0c16ab87bc4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 17120)\n",
            "N√∫mero de documentos: 1617\n",
            "Tamanho do vocabul√°rio (features): 17120\n",
            "\n",
            "Exemplos de features (n-gramas detectados):\n",
            "['abafadi√ßo', 'abafadi√ßo sai', 'abafadi√ßo sai abelha', 'abaixa', 'abaixa rabo', 'abaixa rabo se', 'abaixo', 'abaixo come', 'abandona', 'abarca', 'abarca pouco', 'abarca pouco abra√ßa', 'abastado', 'abastado quem', 'abastado quem d√°', 'abeg√£o', 'abelha', 'abelha do', 'abelha do corti√ßo', 'abelha n√£o', 'abelha n√£o leva', 'abelha sen√£o', 'abelha sen√£o quem', 'abelhas', 'abelhas para', 'abelhas para as', 'aben√ßoada', 'aberta', 'aberta ou', 'aberta ou entra']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Exemplo 1.4 ‚Äì Ajustes avan√ßados em CountVectorizer"
      ],
      "metadata": {
        "id": "fj2-yRnYY2Pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_features:\tDefine un n√∫mero m√°ximo de palabras a conservar. Usa las m√°s frecuentes.\t  max_features=1000 mantiene solo las 1000 palabras m√°s comunes.\n",
        "\n",
        "max_df:\tPalabras que aparecen en demasiados documentos se descartan. Puede ser valor absoluto o proporci√≥n.\t max_df=0.75 ignora palabras que aparecen en ‚â•75 % de los documentos.\n",
        "\n",
        "min_df:\tPalabras que aparecen muy poco tambi√©n se descartan.\tmin_df=5 mantiene solo palabras que aparecen en ‚â•5 documentos.\n",
        "\n",
        "strip_accents:\tElimina acentos antes del an√°lisis.\t\"cora√ß√£o\" ‚Üí \"coracao\"\n",
        "\n",
        "stop_words:\tPermite eliminar stopwords (palabras vac√≠as sin significado, como ‚Äúo‚Äù, ‚Äúde‚Äù, ‚Äúem‚Äù).\tSe puede usar la lista de stopwords del idioma portugu√©s en NLTK."
      ],
      "metadata": {
        "id": "Au7ZeZNupkRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "\n",
        "# Descargamos las stopwords de NLTK (solo la primera vez)\n",
        "nltk.download('stopwords')\n",
        "pt_stop = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador con par√°metros personalizados\n",
        "c_vect = CountVectorizer(\n",
        "    ngram_range=(1, 3),       # unigramas, bigramas y trigramas\n",
        "    strip_accents='unicode',  # elimina acentos\n",
        "    stop_words=pt_stop,       # usa stopwords en portugu√©s\n",
        "    min_df=5,                 # ignora palabras que aparecen en <5 documentos\n",
        "    max_df=0.75,              # ignora palabras que aparecen en >75% de documentos\n",
        "    max_features=1000         # mantiene las 1000 m√°s frecuentes\n",
        ")\n",
        "\n",
        "# Generamos la matriz documento √ó t√©rmino\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Informaci√≥n sobre la matriz\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "print(\"N√∫mero de features finais:\", len(c_vect.get_feature_names_out()))\n",
        "\n",
        "# Visualizamos algunas features seleccionadas\n",
        "print(\"\\nAlgumas features seleccionadas:\")\n",
        "print(list(c_vect.get_feature_names_out())[:30])\n"
      ],
      "metadata": {
        "id": "K5-IldQiqc_w",
        "outputId": "69cdb43c-c55d-4527-a8d6-848b34eeca94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 314)\n",
            "N√∫mero de features finais: 314\n",
            "\n",
            "Algumas features seleccionadas:\n",
            "['abril', 'agua', 'aguas', 'ajuda', 'alcanca', 'alegria', 'alto', 'amanha', 'amigo', 'amigos', 'amizade', 'amor', 'amores', 'anda', 'andar', 'ano', 'antes', 'apanha', 'aprende', 'arvore', 'assim', 'ate', 'baixo', 'barriga', 'basta', 'bebe', 'beleza', 'bem', 'bem nao', 'boa']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ate', 'eramos', 'estao', 'estavamos', 'estiveramos', 'estivessemos', 'foramos', 'fossemos', 'ha', 'hao', 'houveramos', 'houverao', 'houveriamos', 'houvessemos', 'ja', 'nao', 'sao', 'sera', 'serao', 'seriamos', 'so', 'tambem', 'tera', 'terao', 'teriamos', 'tinhamos', 'tiveramos', 'tivessemos', 'voce', 'voces'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Exemplo 1.5 ‚Äì Representa√ß√£o TF-IDF com TfidfVectorizer"
      ],
      "metadata": {
        "id": "4Kc6-uH_ZPwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "Cambiar el modelo de representaci√≥n de texto:\n",
        "üëâ pasar de contar palabras (Bag-of-Words) a ponderarlas seg√∫n su importancia (TF-IDF).\n",
        "\n",
        "Esto mejora el rendimiento de muchos modelos, ya que no todas las palabras tienen el mismo peso en la comprensi√≥n de un texto.\n",
        "\n",
        "üß† Explicaci√≥n te√≥rica\n",
        "\n",
        "El TF-IDF combina dos ideas:\n",
        "\n",
        "TF (Term Frequency) ‚Üí mide cu√°ntas veces aparece una palabra en un documento.\n",
        "\n",
        "Cuanto m√°s aparece, m√°s relevante es para ese documento.\n",
        "\n",
        "Ejemplo: si en un proverbio la palabra ‚Äúvida‚Äù aparece 3 veces, TF(vida)=3.\n",
        "\n",
        "IDF (Inverse Document Frequency) ‚Üí mide en cu√°ntos documentos aparece esa palabra.\n",
        "\n",
        "Cuanto m√°s com√∫n sea en todos los textos, menos informativa es.\n",
        "\n",
        "Ejemplo: si ‚Äúvida‚Äù aparece en 90 % de los proverbios, su IDF ser√° peque√±o.\n",
        "\n",
        "üí¨ Interpretaci√≥n\n",
        "\n",
        "A diferencia de CountVectorizer, aqu√≠ los valores no son enteros, sino ponderaciones decimales (entre 0 y 1).\n",
        "\n",
        "Palabras muy frecuentes en un documento, pero poco comunes en el corpus total, tienen un peso TF-IDF alto.\n",
        "\n",
        "Las palabras comunes (como ‚Äúde‚Äù, ‚Äúo‚Äù, ‚Äúa‚Äù) tienen un peso bajo o nulo.\n",
        "\n",
        "üìà As√≠, la representaci√≥n TF-IDF resalta las palabras m√°s caracter√≠sticas de cada texto."
      ],
      "metadata": {
        "id": "BYfcYn9QrIy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Exemplo 1.5 ‚Äì Representa√ß√£o TF-IDF com *TfidfVectorizer*\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Objetivo\n",
        "Cambiar el modelo de representaci√≥n de texto:\n",
        "\n",
        "üëâ Pasar de **contar palabras (Bag-of-Words)** a **ponderarlas seg√∫n su importancia (TF-IDF)**.  \n",
        "\n",
        "Esto mejora el rendimiento de muchos modelos, ya que no todas las palabras tienen el mismo peso en la comprensi√≥n de un texto.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Explicaci√≥n te√≥rica\n",
        "\n",
        "El **TF-IDF** combina dos ideas fundamentales:\n",
        "\n",
        "#### üî∏ TF (*Term Frequency*)\n",
        "Mide cu√°ntas veces aparece una palabra en un documento.  \n",
        "Cuanto m√°s aparece, m√°s relevante es para ese documento.\n",
        "\n",
        "üìò Ejemplo:  \n",
        "Si en un proverbio la palabra *vida* aparece 3 veces,  \n",
        "entonces TF(vida) = 3.\n",
        "\n",
        "#### üî∏ IDF (*Inverse Document Frequency*)\n",
        "Mide en cu√°ntos documentos aparece esa palabra.  \n",
        "Cuanto m√°s com√∫n sea en todos los textos, menos informativa es.\n",
        "\n",
        "üìò Ejemplo:  \n",
        "Si *vida* aparece en 90 % de los proverbios, su IDF ser√° peque√±o.\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Interpretaci√≥n\n",
        "\n",
        "A diferencia de **CountVectorizer**, aqu√≠ los valores no son enteros,  \n",
        "sino **ponderaciones decimales** (entre 0 y 1).  \n",
        "\n",
        "- Palabras muy frecuentes en un documento, pero poco comunes en el corpus total,  \n",
        "  ‚Üí tienen un peso **TF-IDF alto**.  \n",
        "- Palabras comunes (como *de*, *o*, *a*)  \n",
        "  ‚Üí tienen un peso **bajo o nulo**.  \n",
        "\n",
        "üßæ As√≠, la representaci√≥n **TF-IDF resalta las palabras m√°s caracter√≠sticas** de cada texto.\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ F√≥rmula del c√°lculo TF-IDF\n",
        "\n",
        "El valor **TF-IDF** de un t√©rmino $t$ en un documento $d$ dentro de un corpus de $N$ documentos se calcula como:\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\log \\left( \\frac{N}{\\text{DF}(t)} \\right)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $\\text{TF}(t, d)$ ‚Üí frecuencia del t√©rmino *t* en el documento *d*  \n",
        "- $\\text{DF}(t)$ ‚Üí n√∫mero de documentos que contienen el t√©rmino *t*  \n",
        "- $N$ ‚Üí n√∫mero total de documentos del corpus"
      ],
      "metadata": {
        "id": "xUBR6aChtATF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador TF-IDF\n",
        "tf_vect = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),  # unigramas y bigramas\n",
        "    min_df=3,            # debe aparecer en al menos 3 documentos\n",
        "    max_df=0.5,          # y en no m√°s del 50% de los documentos\n",
        "    max_features=500     # m√°ximo de 500 features\n",
        ")\n",
        "\n",
        "# Generamos la matriz TF-IDF\n",
        "p_tf = tf_vect.fit_transform(docs)\n",
        "\n",
        "# Mostramos resultados b√°sicos\n",
        "print(\"Forma da matriz:\", p_tf.shape)\n",
        "print(\"N√∫mero de features:\", len(tf_vect.get_feature_names_out()))\n",
        "\n",
        "# Mostramos algunas features y valores TF-IDF del primer documento\n",
        "print(\"\\nFeatures (palabras o n-gramas):\")\n",
        "print(list(tf_vect.get_feature_names_out())[:20])\n",
        "\n",
        "print(\"\\nRepresenta√ß√£o do primeiro documento (valores TF-IDF):\")\n",
        "print(p_tf[0, :].toarray().tolist())\n"
      ],
      "metadata": {
        "id": "dDC__DCntSY3",
        "outputId": "4a9a3094-b05d-43c5-b2c7-5839eb4ecff6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 500)\n",
            "N√∫mero de features: 500\n",
            "\n",
            "Features (palabras o n-gramas):\n",
            "['abril', 'acaba', 'ainda', 'ajuda', 'alcan√ßa', 'alegria', 'alheia', 'alheio', 'alma', 'alto', 'ama', 'amanh√£', 'ambi√ß√£o', 'amigo', 'amigos', 'amizade', 'amor', 'amores', 'anda', 'andar']\n",
            "\n",
            "Representa√ß√£o do primeiro documento (valores TF-IDF):\n",
            "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9382731425466352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34589522976713843, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Exemplo 1.6 ‚Äì Reutilizar um Vectorizer com novos documentos"
      ],
      "metadata": {
        "id": "1ET0SEztZ9Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "Aprender a representar nuevos documentos (no vistos durante el entrenamiento) usando el mismo vectorizador ya entrenado.\n",
        "Esto es fundamental cuando trabajas con modelos de aprendizaje autom√°tico, ya que:\n",
        "\n",
        "‚ö†Ô∏è El vocabulario del modelo debe mantenerse constante.\n",
        "Si reentrenas el vectorizador con nuevos textos, el vocabulario puede cambiar,\n",
        "y los vectores dejar√≠an de ser comparables.\n",
        "\n",
        "üß† Explicaci√≥n te√≥rica\n",
        "\n",
        "Cuando entrenamos un CountVectorizer o TfidfVectorizer, ocurren dos pasos:\n",
        "\n",
        "M√©todo\tQu√© hace\n",
        "fit_transform(docs)\tAprende el vocabulario del corpus y transforma los documentos en vectores. (Se usa con los datos de entrenamiento).\n",
        "transform(new_docs)\tUsa el vocabulario aprendido previamente para transformar nuevos documentos.\n",
        "\n",
        "üí° Es decir:\n",
        "\n",
        "fit_transform() se usa una sola vez durante el entrenamiento.\n",
        "\n",
        "transform() se usa cada vez que quieres representar nuevos textos con el mismo modelo"
      ],
      "metadata": {
        "id": "bPGPZhBXtk5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargamos los proverbios (entrenamiento)\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos y entrenamos el vectorizador\n",
        "tf_vect = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),   # unigramas y bigramas\n",
        "    min_df=3,             # debe aparecer en al menos 3 documentos\n",
        "    max_df=0.5,           # no m√°s del 50% de los documentos\n",
        "    max_features=500      # m√°ximo de 500 palabras o n-gramas\n",
        ")\n",
        "p_tf = tf_vect.fit_transform(docs)\n",
        "\n",
        "# Nuevos documentos (no vistos durante el entrenamiento)\n",
        "novos_docs = [\n",
        "    \"amor √© fogo que arde sem se ver\",\n",
        "    \"verdes s√£o os campos\"\n",
        "]\n",
        "\n",
        "# Transformamos los nuevos documentos con el mismo vectorizador\n",
        "novos_vects = tf_vect.transform(novos_docs)\n",
        "\n",
        "# Exploramos la matriz resultante\n",
        "print(\"Forma da matriz dos novos documentos:\", novos_vects.shape)\n",
        "print(\"\\nRepresenta√ß√£o TF-IDF dos novos documentos:\")\n",
        "print(novos_vects.toarray())\n",
        "\n",
        "\n",
        "#üßæ Explicaci√≥n\n",
        "  # tf_vect.fit_transform(docs) ‚Üí se usa solo con los proverbios iniciales (entrenamiento).\n",
        "  # tf_vect.transform(novos_docs) ‚Üí aplica el mismo vocabulario al nuevo texto.\n",
        "  # Palabras que no estaban en el vocabulario original ser√°n ignoradas.\n",
        "\n",
        "# Output es un array (2, 500) donde solo hay decimales en aquellas posiciones de las palbras del documento que marcan su importancia (de 0 a 1) y 0 en el resto, las palabras que no esten en el modelo del vocubulario\n",
        "# inicial entonces ser√°n obviadas\n"
      ],
      "metadata": {
        "id": "XIh9nO6HuD3Q",
        "outputId": "09fc9aee-963c-45a3-f849-52ccdf3bf67c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz dos novos documentos: (2, 500)\n",
            "\n",
            "Representa√ß√£o TF-IDF dos novos documentos:\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.46215083 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.50949558\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.21335991 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.23497398 0.\n",
            "  0.         0.         0.         0.         0.         0.38080138\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.53017539 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.64998539 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.7599467\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 1.3 Desafio ‚Äì Similaridade do Cosseno entre documentos"
      ],
      "metadata": {
        "id": "RhdQyMwlawWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Objetivo\n",
        "\n",
        "El desaf√≠o propone que crees una funci√≥n en Python que, dada una colecci√≥n de documentos y un nuevo documento, encuentre los n documentos m√°s parecidos seg√∫n la similaridad del coseno.\n",
        "\n",
        "Esto te permitir√° medir qu√© tan similares son dos textos bas√°ndote en su representaci√≥n num√©rica (por ejemplo, TF-IDF).\n",
        "\n",
        "üß† Explicaci√≥n te√≥rica\n",
        "\n",
        "Cuando representamos los documentos con TF-IDF, cada documento se convierte en un vector de caracter√≠sticas num√©ricas.\n",
        "Para medir la semejanza entre dos documentos, usamos la similaridad del coseno (cosine similarity)."
      ],
      "metadata": {
        "id": "qojDSanKw65O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPCION 1 (CHATI)"
      ],
      "metadata": {
        "id": "vauYf5Luzniw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1Ô∏è‚É£ Cargar los documentos base\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# 2Ô∏è‚É£ Crear y entrenar el vectorizador TF-IDF\n",
        "tf_vect = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.5)\n",
        "tf_matrix = tf_vect.fit_transform(docs)\n",
        "\n",
        "# 3Ô∏è‚É£ Definir la funci√≥n que devuelve los n documentos m√°s similares\n",
        "def documentos_parecidos(tf_vect, tf_matrix, novo_doc, n=3):\n",
        "    \"\"\"\n",
        "    Dado un vectorizador TF-IDF entrenado, su matriz y un nuevo documento,\n",
        "    devuelve los n documentos m√°s parecidos seg√∫n la similaridad del coseno.\n",
        "    \"\"\"\n",
        "    # Transformar el nuevo documento al mismo espacio vectorial\n",
        "    novo_vec = tf_vect.transform([novo_doc])\n",
        "\n",
        "    # Calcular la similaridad del coseno con todos los documentos\n",
        "    similaridades = cosine_similarity(novo_vec, tf_matrix)[0]\n",
        "\n",
        "    # Obtener los √≠ndices de los documentos m√°s similares\n",
        "    indices = np.argsort(similaridades)[::-1][:n]\n",
        "\n",
        "    # Devolver los documentos y sus puntuaciones\n",
        "    resultados = [(docs[i], similaridades[i]) for i in indices]\n",
        "    return resultados\n",
        "\n",
        "# 4Ô∏è‚É£ Probar la funci√≥n con un nuevo texto\n",
        "novo_texto = \"quem espera sempre alcan√ßa\"\n",
        "resultados = documentos_parecidos(tf_vect, tf_matrix, novo_texto, n=5)\n",
        "\n",
        "print(\"üîé Documentos mais semelhantes a:\", novo_texto)\n",
        "for doc, sim in resultados:\n",
        "    print(f\"{sim:.3f} ‚Üí {doc}\")\n"
      ],
      "metadata": {
        "id": "QJ0HPoyha02y",
        "outputId": "7d755ae8-5a5c-42c8-d009-80cbc26c1c89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Documentos mais semelhantes a: quem espera sempre alcan√ßa\n",
            "1.000 ‚Üí quem espera sempre alcan√ßa.\n",
            "0.643 ‚Üí quem porfia sempre alcan√ßa.\n",
            "0.577 ‚Üí quem tem esperan√ßa sempre alcan√ßa.\n",
            "0.547 ‚Üí quem espera desespera.\n",
            "0.420 ‚Üí quem espera por sapatos de defunto, toda a vida anda descal√ßo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPCION 2 (CLASE)"
      ],
      "metadata": {
        "id": "0v4tYgn5zqTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar(docs, doc, n=10, smin=0.5):\n",
        "    tf_vect = TfidfVectorizer(ngram_range=(1,3), strip_accents='unicode',\n",
        "                              max_features=500, min_df=5, max_df=0.75)\n",
        "\n",
        "    vects = tf_vect.fit_transform(docs)\n",
        "    doc_vec = tf_vect.transform([doc])\n",
        "\n",
        "    vocabulary = tf_vect.get_feature_names_out()\n",
        "    for i in doc_vec.indices:\n",
        "        print(i, vocabulary[i])\n",
        "\n",
        "    # c√°lculo da similaridade\n",
        "    cosine_matrix = cosine_similarity(vects, doc_vec)\n",
        "    print(cosine_matrix.shape)     # Genera una matriz de tama√±o (n_docs, 1) con la similitud entre cada documento del corpus y el nuevo documento.\n",
        "\n",
        "    lista_cos = [(docs[i], e[0]) for i, e in enumerate(cosine_matrix)]\n",
        "    lista_cos.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [(d, c) for d, c in lista_cos[:n] if c > smin]\n",
        "\n",
        "doc = 'Em terra de cegos, quem tem um olho √© rei.'\n",
        "similar = most_similar(docs, doc)\n",
        "print(similar)"
      ],
      "metadata": {
        "id": "zcz8xjXIzs90",
        "outputId": "64dc7530-b811-4cea-8c35-3c07f96600c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85 de\n",
            "114 em\n",
            "116 em terra\n",
            "262 olho\n",
            "319 quem\n",
            "331 quem tem\n",
            "337 rei\n",
            "383 tem\n",
            "390 terra\n",
            "391 terra de\n",
            "406 um\n",
            "(1617, 1)\n",
            "[('em terra de cegos, quem tem um olho √© rei.', np.float64(1.0)), ('em terra de sapos, de c√≥coras como eles.', np.float64(0.6338665651998857)), ('em terra de saci, cal√ßa comprida d√° pra dois.', np.float64(0.6058372801802083)), ('em terra de sapo, mosquito n√£o d√° rasante.', np.float64(0.5772407093557776))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2¬∫ PARTE DE LA PR√ÅCTICA: Classifica√ß√£o de texto com Scikit-Learn\n"
      ],
      "metadata": {
        "id": "3OSkNKoSNkW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo es crear o cargar un conjunto de documentos de texto que tengan categor√≠as (clases) asociadas, y preparar los datos para entrenar un modelo de clasificaci√≥n de texto.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "publico.txt ‚Üí contiene t√≠tulos de noticias normales.\n",
        "\n",
        "ipublico.txt ‚Üí contiene t√≠tulos humor√≠sticos del suplemento Inimigo P√∫blico.\n",
        "\n",
        "Cada l√≠nea del archivo representa un documento (un texto corto)."
      ],
      "metadata": {
        "id": "_LtHzCFsNsMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/TitulosPublico_N.txt -O publico.txt\n",
        "!wget https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/inimigopublico_H.txt -O ipublico.txt\n"
      ],
      "metadata": {
        "id": "Rxs0XU5aNygc",
        "outputId": "43a1e8bc-970c-49be-ce95-b2ab9e676d91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-07 10:28:52--  https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/TitulosPublico_N.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 159919 (156K) [text/plain]\n",
            "Saving to: ‚Äòpublico.txt‚Äô\n",
            "\n",
            "\rpublico.txt           0%[                    ]       0  --.-KB/s               \rpublico.txt         100%[===================>] 156.17K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-10-07 10:28:52 (77.7 MB/s) - ‚Äòpublico.txt‚Äô saved [159919/159919]\n",
            "\n",
            "--2025-10-07 10:28:52--  https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/inimigopublico_H.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 222678 (217K) [text/plain]\n",
            "Saving to: ‚Äòipublico.txt‚Äô\n",
            "\n",
            "ipublico.txt        100%[===================>] 217.46K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2025-10-07 10:28:52 (60.8 MB/s) - ‚Äòipublico.txt‚Äô saved [222678/222678]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Objetivo\n",
        "\n",
        "El prop√≥sito de este ejemplo es:\n",
        "\n",
        "Cargar los documentos de texto desde los ficheros descargados (publico.txt y ipublico.txt).\n",
        "\n",
        "Crear una lista de etiquetas (clases) para cada documento:\n",
        "\n",
        "'P' ‚Üí t√≠tulo del P√∫blico\n",
        "\n",
        "'IP' ‚Üí t√≠tulo del Inimigo P√∫blico"
      ],
      "metadata": {
        "id": "8b2c7uA3OG0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Exemplo 1.7 - Cargar documentos y clases\n",
        "# =====================================================\n",
        "\n",
        "# Funci√≥n para cargar documentos desde un fichero de texto\n",
        "coment = '#'\n",
        "\n",
        "def carrega_docs(ficheiro):\n",
        "    docs = []\n",
        "    with open(ficheiro, encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for txt in lines:\n",
        "            txt = txt.strip()\n",
        "            if len(txt) > 0 and txt[0] != coment:\n",
        "                docs.append(txt)\n",
        "    return docs\n",
        "\n",
        "# Cargar los t√≠tulos del P√∫blico e Inimigo P√∫blico\n",
        "docs_p = carrega_docs('publico.txt')\n",
        "docs_ip = carrega_docs('ipublico.txt')\n",
        "\n",
        "# Crear lista de textos (X) y sus etiquetas (y)\n",
        "lista_X = docs_p + docs_ip\n",
        "lista_y = ['P' if i < len(docs_p) else 'IP' for i in range(len(lista_X))]\n",
        "\n",
        "# Comprobaci√≥n\n",
        "print(f\"N√∫mero total de documentos: {len(lista_X)}\")\n",
        "print(f\"N√∫mero de etiquetas: {len(lista_y)}\")\n",
        "print(f\"Ejemplo documento: {lista_X[0]}\")\n",
        "print(f\"Clase asociada: {lista_y[0]}\")"
      ],
      "metadata": {
        "id": "yz0CsHbGOMUn",
        "outputId": "3e05b2fc-9566-40e4-fea3-afe86f82bf34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N√∫mero total de documentos: 4480\n",
            "N√∫mero de etiquetas: 4480\n",
            "Ejemplo documento: Os maiores esc√¢ndalos de abuso sexual na Igreja desde a elei√ß√£o do Papa Francisco.\n",
            "Clase asociada: P\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entonces estamos juntando todo en un mismo archivo pero cada documento tiene una etiqueta segun si proceda de publico.txt o ipublico.txt"
      ],
      "metadata": {
        "id": "KkzQy_vwO2Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© Exemplo 1.8 ‚Äî Divis√£o treino/teste"
      ],
      "metadata": {
        "id": "4ARpD4uVO-PL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de este paso es dividir nuestro conjunto de datos (lista_X y lista_y) en dos partes:\n",
        "\n",
        "Conjunto de entrenamiento (train):\n",
        "Usado para entrenar el modelo (ajustar sus par√°metros).\n",
        "\n",
        "Conjunto de prueba (test):\n",
        "Usado para evaluar el rendimiento del modelo con datos nuevos, es decir, textos que el modelo no ha visto durante el entrenamiento"
      ],
      "metadata": {
        "id": "ZBxy9CbtPClO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Exemplo 1.8 - Separar dados em treino e teste\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividir os dados em treino (70%) e teste (30%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    lista_X, lista_y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"Total de documentos: {len(lista_X)}\")\n",
        "print(f\"Treino: {len(X_train)} documentos\")\n",
        "print(f\"Teste:  {len(X_test)} documentos\")\n",
        "\n",
        "# Ejemplo de un t√≠tulo y su clase\n",
        "print(\"\\nExemplo de treino:\")\n",
        "print(X_train[0])\n",
        "print(f\"Classe: {y_train[0]}\")"
      ],
      "metadata": {
        "id": "-2gz3dgcO-9u",
        "outputId": "439f36cc-b079-46eb-879e-551dda72e643",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de documentos: 4480\n",
            "Treino: 3136 documentos\n",
            "Teste:  1344 documentos\n",
            "\n",
            "Exemplo de treino:\n",
            "Quantos mais super-her√≥is cabem no guarda-chuva das s√©ries?.\n",
            "Classe: P\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© Exemplo 1.9 ‚Äî Representaci√≥n de los documentos"
      ],
      "metadata": {
        "id": "35fbhBIgPJkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Objetivo\n",
        "\n",
        "Transformar nuestros textos (strings) en vectores num√©ricos, que es la √∫nica forma en que los algoritmos de Machine Learning pueden trabajar.\n",
        "\n",
        "‚öôÔ∏è Recordatorio\n",
        "\n",
        "Hasta ahora tenemos:\n",
        "\n",
        "X_train, X_test ‚Üí los textos.\n",
        "\n",
        "y_train, y_test ‚Üí las etiquetas (‚ÄòP‚Äô o ‚ÄòIP‚Äô).\n",
        "\n",
        "Ahora debemos convertir los textos en n√∫meros usando un vectorizador."
      ],
      "metadata": {
        "id": "NqZfLqpuPhkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Exemplo 1.9 - Representa√ß√£o de texto com CountVectorizer\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Crear el vectorizador\n",
        "count_vect = CountVectorizer()\n",
        "\n",
        "# Ajustar el vectorizador al conjunto de treino\n",
        "count_vects_train = count_vect.fit_transform(X_train)\n",
        "\n",
        "# Transformar el conjunto de teste usando el mismo vocabulario\n",
        "count_vects_test = count_vect.transform(X_test)\n",
        "\n",
        "# Mostrar informaci√≥n\n",
        "print(\"Forma da matriz de treino:\", count_vects_train.shape)\n",
        "print(\"Forma da matriz de teste:\", count_vects_test.shape)\n",
        "\n",
        "# Ver algunas features (palabras del vocabulario)\n",
        "print(\"\\nExemplo de features extra√≠das:\")\n",
        "print(count_vect.get_feature_names_out()[:20])\n"
      ],
      "metadata": {
        "id": "bWoTCKhhPfU_",
        "outputId": "838c4286-39a2-430d-d622-e938327cdca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz de treino: (3136, 8870)\n",
            "Forma da matriz de teste: (1344, 8870)\n",
            "\n",
            "Exemplo de features extra√≠das:\n",
            "['000' '01' '10' '100' '103' '107' '10yearchallenge' '11' '110' '1111'\n",
            " '112' '12' '13' '130' '1300' '132' '134' '14' '140' '141']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© Exemplo 1.10 ‚Äî Entrenamiento de un clasificador Naive Bayes\n",
        "üß† Objetivo\n",
        "\n",
        "Usar los vectores num√©ricos que creamos con CountVectorizer para entrenar un modelo de clasificaci√≥n de texto.\n",
        "En este caso se usa el Naive Bayes Multinomial, un algoritmo cl√°sico y muy eficiente para tareas de clasificaci√≥n de texto (como detecci√≥n de spam, an√°lisis de sentimientos, o detecci√≥n de humor).\n",
        "\n",
        "‚öôÔ∏è ¬øPor qu√© Naive Bayes?\n",
        "En pocas palabras: estima la probabilidad de que un texto pertenezca a una clase bas√°ndose en las palabras que contiene."
      ],
      "metadata": {
        "id": "mWAvqkNsPkZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Exemplo 1.10 - Treinar um classificador Naive Bayes\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Crear y entrenar el clasificador\n",
        "clf = MultinomialNB().fit(count_vects_train, y_train)\n",
        "\n",
        "# Predecir las clases del conjunto de teste\n",
        "predicted = clf.predict(count_vects_test)\n",
        "\n",
        "# Mostrar algunas predicciones\n",
        "for doc, category, true_label in zip(X_test[:5], predicted[:5], y_test[:5]):\n",
        "    print(f\"{doc!r} => Predicho: {category} | Real: {true_label}\")\n"
      ],
      "metadata": {
        "id": "NxYbFJ4zP0mP",
        "outputId": "8c741677-471a-4594-fb6d-682ddca40bff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Professores doentes est√£o a ser obrigados a regressar √†s escolas.' => Predicho: P | Real: P\n",
            "'Prendas de ofere√ßa um banco portugu√™s √† av√≥' => Predicho: IP | Real: IP\n",
            "'Tribunal de recurso mant√©m hacker Rui Pinto em pris√£o domicili√°ria.' => Predicho: P | Real: P\n",
            "'Minist√©rio P√∫blico: ‚ÄúO problema √© o controlo sindical, n√£o o controlo pol√≠tico‚Äù.' => Predicho: P | Real: P\n",
            "'Quando as europeias foram um \"trinta e um\" para o PS.' => Predicho: P | Real: P\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© Exemplo 1.11 ‚Äî Avalia√ß√£o do classificador\n",
        "üß† Objetivo\n",
        "\n",
        "El objetivo de este paso es evaluar cuantitativamente c√≥mo de bien est√° funcionando nuestro modelo.\n",
        "Para ello usaremos las m√©tricas m√°s comunes en clasificaci√≥n:"
      ],
      "metadata": {
        "id": "vO-sCl63P5OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìò Explicaci√≥n r√°pida\n",
        "\n",
        "metrics.classification_report(y_test, predicted, target_names=set(lista_y))\n",
        "‚Üí muestra un resumen de precisi√≥n, exhaustividad y F1 para cada clase (‚ÄòP‚Äô e ‚ÄòIP‚Äô).\n",
        "\n",
        "metrics.confusion_matrix(y_test, predicted)\n",
        "‚Üí muestra cu√°ntas predicciones fueron correctas o err√≥neas para cada clase."
      ],
      "metadata": {
        "id": "NaamUmZeQDGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Exemplo 1.11 - Avalia√ß√£o do classificador\n",
        "# =====================================================\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "# Relat√≥rio de m√©tricas detalhado\n",
        "print(metrics.classification_report(\n",
        "    y_test, predicted, target_names=set(lista_y))\n",
        ")\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "print(\"Matriz de confus√£o:\")\n",
        "print(metrics.confusion_matrix(y_test, predicted))\n"
      ],
      "metadata": {
        "id": "ubjETkq3QDzk",
        "outputId": "e5d6a852-2372-4b0a-8a11-b8f8a3622f3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           P       0.74      0.89      0.81       667\n",
            "          IP       0.86      0.69      0.76       677\n",
            "\n",
            "    accuracy                           0.79      1344\n",
            "   macro avg       0.80      0.79      0.79      1344\n",
            "weighted avg       0.80      0.79      0.79      1344\n",
            "\n",
            "Matriz de confus√£o:\n",
            "[[593  74]\n",
            " [212 465]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# La parte de support te dice la cantidad de muestras para cada clase --> nos puede servir para saber si las clases estan balanceadas o no"
      ],
      "metadata": {
        "id": "Kz7NdS2jUP3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© 1.6 Exerc√≠cios ‚Äî Objetivo general\n",
        "\n",
        "El objetivo de esta parte es experimentar y comparar distintos:\n",
        "\n",
        "Vectorizadores (CountVectorizer, TfidfVectorizer)\n",
        "\n",
        "Modelos clasificadores (Naive Bayes, SVM, Logistic Regression, etc.)\n",
        "\n",
        "Par√°metros de configuraci√≥n (n-gramas, min_df, max_df, etc.)\n",
        "\n",
        "üëâ La meta es entender c√≥mo cambian los resultados (accuracy, precision, recall, f1) al modificar las representaciones o los clasificadores."
      ],
      "metadata": {
        "id": "iRDR67oySvkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Exerc√≠cio 1.2 ‚Äî Experimentar con el vectorizador\n",
        "üéØ Qu√© pide\n",
        "\n",
        "Analizar el impacto de cambiar los par√°metros del vectorizador:\n",
        "\n",
        "ngram_range ‚Üí cambia el tama√±o de las secuencias de palabras.\n",
        "\n",
        "min_df ‚Üí ignora palabras que aparecen en pocos documentos.\n",
        "\n",
        "max_df ‚Üí ignora palabras que aparecen en casi todos los documentos.\n",
        "\n",
        "max_features ‚Üí limita el n√∫mero de palabras del vocabulario.\n",
        "\n",
        "Tambi√©n te pide probar otro tipo de vectorizador: el TfidfVectorizer."
      ],
      "metadata": {
        "id": "CjqHAV5OSykv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# Crear un vectorizador TF-IDF con diferentes par√°metros\n",
        "tfidf_vect = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),   # usa unigramas y bigramas\n",
        "    min_df=3,             # palabra debe aparecer al menos en 3 documentos\n",
        "    max_df=0.7,           # ignora palabras muy frecuentes\n",
        "    max_features=1000     # limita el vocabulario\n",
        ")\n",
        "\n",
        "# Transformar los textos\n",
        "tfidf_train = tfidf_vect.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vect.transform(X_test)\n",
        "\n",
        "# Entrenar y evaluar\n",
        "clf = MultinomialNB().fit(tfidf_train, y_train)\n",
        "pred = clf.predict(tfidf_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, pred))"
      ],
      "metadata": {
        "id": "S62A3jUqS1Ng",
        "outputId": "701cbf58-993d-4116-9a8b-8087e2427163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.76      0.73      0.74       667\n",
            "           P       0.74      0.77      0.76       677\n",
            "\n",
            "    accuracy                           0.75      1344\n",
            "   macro avg       0.75      0.75      0.75      1344\n",
            "weighted avg       0.75      0.75      0.75      1344\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Ejercicio 1.2 - Comparando CountVectorizer con distintos par√°metros\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# 1Ô∏è‚É£ Vectorizador con unigramas (solo palabras individuales)\n",
        "cv1 = CountVectorizer(ngram_range=(1,1))\n",
        "X_train_cv1 = cv1.fit_transform(X_train)\n",
        "X_test_cv1 = cv1.transform(X_test)\n",
        "\n",
        "clf1 = MultinomialNB().fit(X_train_cv1, y_train)\n",
        "pred1 = clf1.predict(X_test_cv1)\n",
        "print(\"=== Modelo 1: Unigramas ===\")\n",
        "print(metrics.classification_report(y_test, pred1))\n",
        "\n",
        "# 2Ô∏è‚É£ Vectorizador con unigramas y bigramas (considera pares de palabras)\n",
        "cv2 = CountVectorizer(ngram_range=(1,2), max_features=2000, max_df=0.75, min_df=3)\n",
        "X_train_cv2 = cv2.fit_transform(X_train)\n",
        "X_test_cv2 = cv2.transform(X_test)\n",
        "\n",
        "clf2 = MultinomialNB().fit(X_train_cv2, y_train)\n",
        "pred2 = clf2.predict(X_test_cv2)\n",
        "print(\"\\n=== Modelo 2: Unigramas + Bigramas ===\")\n",
        "print(metrics.classification_report(y_test, pred2))\n",
        "\n",
        "# 3Ô∏è‚É£ Vectorizador con limpieza adicional (sin acentos y stopwords en portugu√©s)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "pt_stop = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "cv3 = CountVectorizer(\n",
        "    ngram_range=(1,2),\n",
        "    strip_accents='unicode',\n",
        "    stop_words=pt_stop,\n",
        "    min_df=5,\n",
        "    max_df=0.7,\n",
        "    max_features=1500\n",
        ")\n",
        "\n",
        "X_train_cv3 = cv3.fit_transform(X_train)\n",
        "X_test_cv3 = cv3.transform(X_test)\n",
        "\n",
        "clf3 = MultinomialNB().fit(X_train_cv3, y_train)\n",
        "pred3 = clf3.predict(X_test_cv3)\n",
        "print(\"\\n=== Modelo 3: Bigramas + Stopwords removidas + Acentos normalizados ===\")\n",
        "print(metrics.classification_report(y_test, pred3))"
      ],
      "metadata": {
        "id": "V87GFbFsT-4C",
        "outputId": "728a13d8-ee4a-428b-cc73-66b8b0f37256",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Modelo 1: Unigramas ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.74      0.89      0.81       667\n",
            "           P       0.86      0.69      0.76       677\n",
            "\n",
            "    accuracy                           0.79      1344\n",
            "   macro avg       0.80      0.79      0.79      1344\n",
            "weighted avg       0.80      0.79      0.79      1344\n",
            "\n",
            "\n",
            "=== Modelo 2: Unigramas + Bigramas ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.77      0.75      0.76       667\n",
            "           P       0.76      0.78      0.77       677\n",
            "\n",
            "    accuracy                           0.77      1344\n",
            "   macro avg       0.77      0.77      0.77      1344\n",
            "weighted avg       0.77      0.77      0.77      1344\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ate', 'eramos', 'estao', 'estavamos', 'estiveramos', 'estivessemos', 'foramos', 'fossemos', 'ha', 'hao', 'houveramos', 'houverao', 'houveriamos', 'houvessemos', 'ja', 'nao', 'sao', 'sera', 'serao', 'seriamos', 'so', 'tambem', 'tera', 'terao', 'teriamos', 'tinhamos', 'tiveramos', 'tivessemos', 'voce', 'voces'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Modelo 3: Bigramas + Stopwords removidas + Acentos normalizados ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.76      0.77      0.76       667\n",
            "           P       0.77      0.76      0.77       677\n",
            "\n",
            "    accuracy                           0.77      1344\n",
            "   macro avg       0.77      0.77      0.77      1344\n",
            "weighted avg       0.77      0.77      0.77      1344\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Exerc√≠cio 1.3 ‚Äî Probar distintos clasificadores\n",
        "üéØ Qu√© pide\n",
        "\n",
        "Entrenar y comparar otros modelos, cambiando solo el clasificador, mientras mantienes la misma representaci√≥n vectorial.\n",
        "\n",
        "Modelos sugeridos:\n",
        "\n",
        "sklearn.linear_model.LogisticRegression\n",
        "\n",
        "sklearn.neighbors.KNeighborsClassifier\n",
        "\n",
        "sklearn.svm.SVC (m√°quinas de soporte vectorial)\n",
        "\n",
        "sklearn.ensemble.RandomForestClassifier"
      ],
      "metadata": {
        "id": "My1AcLaaS2I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "# Usar el mismo vectorizador que antes\n",
        "X_train_vec = count_vects_train\n",
        "X_test_vec = count_vects_test\n",
        "\n",
        "# 1Ô∏è‚É£ Logistic Regression\n",
        "logreg = LogisticRegression(max_iter=1000).fit(X_train_vec, y_train)\n",
        "pred_log = logreg.predict(X_test_vec)\n",
        "print(\"Logistic Regression:\\n\", metrics.classification_report(y_test, pred_log))\n",
        "\n",
        "# 2Ô∏è‚É£ Support Vector Machine\n",
        "svm = SVC().fit(X_train_vec, y_train)\n",
        "pred_svm = svm.predict(X_test_vec)\n",
        "print(\"\\nSVM:\\n\", metrics.classification_report(y_test, pred_svm))\n",
        "\n",
        "# 3Ô∏è‚É£ Random Forest\n",
        "rf = RandomForestClassifier().fit(X_train_vec, y_train)\n",
        "pred_rf = rf.predict(X_test_vec)\n",
        "print(\"\\nRandom Forest:\\n\", metrics.classification_report(y_test, pred_rf))"
      ],
      "metadata": {
        "id": "xJIxo-qeS48j",
        "outputId": "558dd956-a9ce-4282-cc19-12f0ef030c44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.89      0.77      0.83       667\n",
            "           P       0.80      0.91      0.85       677\n",
            "\n",
            "    accuracy                           0.84      1344\n",
            "   macro avg       0.85      0.84      0.84      1344\n",
            "weighted avg       0.85      0.84      0.84      1344\n",
            "\n",
            "\n",
            "SVM:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.93      0.68      0.79       667\n",
            "           P       0.75      0.95      0.84       677\n",
            "\n",
            "    accuracy                           0.82      1344\n",
            "   macro avg       0.84      0.81      0.81      1344\n",
            "weighted avg       0.84      0.82      0.81      1344\n",
            "\n",
            "\n",
            "Random Forest:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.92      0.65      0.76       667\n",
            "           P       0.73      0.95      0.83       677\n",
            "\n",
            "    accuracy                           0.80      1344\n",
            "   macro avg       0.83      0.80      0.79      1344\n",
            "weighted avg       0.83      0.80      0.79      1344\n",
            "\n"
          ]
        }
      ]
    }
  ]
}