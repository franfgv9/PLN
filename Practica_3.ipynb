{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2grv0CXSR1blu2xFhx5Gh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franfgv9/PLN/blob/main/Practica_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5KhW5wcJ9CL",
        "outputId": "555aa56d-aa12-4aa3-f44e-a7211dd4159f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-07 10:25:35--  https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/proverbios_N.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68195 (67K) [text/plain]\n",
            "Saving to: ‘proverbios.txt’\n",
            "\n",
            "\rproverbios.txt        0%[                    ]       0  --.-KB/s               \rproverbios.txt      100%[===================>]  66.60K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-10-07 10:25:35 (34.3 MB/s) - ‘proverbios.txt’ saved [68195/68195]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/proverbios_N.txt -O proverbios.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "El objetivo inicial es crear o cargar una colección de documentos de texto y representarlos como una lista de cadenas de texto en Python.\n",
        "Cada línea de texto del fichero representará un documento (por ejemplo, un proverbio o una frase).\n",
        "\n",
        "🧠 Explicación teórica\n",
        "\n",
        "El primer paso en cualquier proyecto de procesamiento de lenguaje natural es obtener los datos textuales."
      ],
      "metadata": {
        "id": "-3fLPnfUkLQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentario = '#'\n",
        "def carrega_docs ( ficheiro ) :\n",
        "  docs = []\n",
        "  with open ( ficheiro , encoding ='utf -8 ') as f :\n",
        "    lines = f . readlines ()\n",
        "    for txt in lines :\n",
        "      txt = txt . strip ()\n",
        "      if len ( txt ) > 0 and txt [0] != comentario :\n",
        "        docs . append ( txt )\n",
        "  return docs\n",
        "\n",
        "# Se abre el fichero (open(ficheiro, encoding='utf-8'))\n",
        "# Se leen todas las líneas (readlines())\n",
        "# Se limpian los espacios y saltos de línea (strip())\n",
        "# Se ignoran comentarios o líneas vacías\n",
        "# Finalmente, se devuelve una lista de documentos, donde cada elemento es una cadena de texto (un documento independiente).\n"
      ],
      "metadata": {
        "id": "7GtaW4QFU6Vr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Exemplo 1.2 – Bag of Words (Bolsa de Palavras)"
      ],
      "metadata": {
        "id": "5Zz5lyzNW-PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "Transformar una colección de textos en una matriz numérica que indique cuántas veces aparece cada palabra en cada documento.\n",
        "Esta representación es fundamental porque los algoritmos de Machine Learning no pueden trabajar directamente con texto: necesitan números.\n",
        "\n",
        "\n",
        "🧠 Explicación teórica\n",
        "\n",
        "El modelo Bag-of-Words (BoW) consiste en:\n",
        "\n",
        "  1. Crear un vocabulario con todas las palabras únicas del corpus.\n",
        "\n",
        "  2. Representar cada documento como un vector de frecuencias de esas palabras.\n",
        "\n",
        "📊 Cada fila = un documento\n",
        "📈 Cada columna = una palabra del vocabulario\n",
        "💾 Cada valor = número de veces que la palabra aparece en el documento"
      ],
      "metadata": {
        "id": "pPyhq46-ksdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Cargamos los documentos (por ejemplo, los proverbios)\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador\n",
        "c_vect = CountVectorizer()\n",
        "\n",
        "# Transformamos los documentos en una matriz documento × término\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Visualizamos la forma de la matriz (nº de documentos, nº de palabras)\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "\n",
        "# Obtenemos el vocabulario (todas las palabras únicas)\n",
        "print(\"Vocabulario:\", c_vect.get_feature_names_out())\n",
        "\n",
        "# Visualizamos el vector del primer documento\n",
        "print(\"Representação do primeiro documento:\")\n",
        "print(p_counts[0, :].toarray().tolist())\n",
        "print(p_counts[0, :])\n",
        "print(docs[0])\n"
      ],
      "metadata": {
        "id": "tD6JqH3JluCL",
        "outputId": "a5e16c91-098e-439a-dcc3-7dc8cafe95ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 2741)\n",
            "Vocabulario: ['abafadiço' 'abaixa' 'abaixo' ... 'último' 'últimos' 'útil']\n",
            "Representação do primeiro documento:\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 4 stored elements and shape (1, 2741)>\n",
            "  Coords\tValues\n",
            "  (0, 7)\t1\n",
            "  (0, 1767)\t1\n",
            "  (0, 1430)\t1\n",
            "  (0, 546)\t1\n",
            "a abelha não leva chumbo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Exemplo 1.3 – N-gramas com CountVectorizer"
      ],
      "metadata": {
        "id": "4rrCX0Z3XABF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "Mejorar la representación del texto considerando no solo palabras individuales, sino también secuencias de palabras consecutivas (los llamados n-gramas).\n",
        "Esto permite capturar expresiones y contextos que el modelo Bag-of-Words simple no detecta.\n",
        "\n",
        "🔍 ¿Por qué usar n-gramas?\n",
        "\n",
        "Unigramas capturan las palabras por separado (útiles para vocabularios amplios\n",
        "\n",
        "Bigramas o trigramas capturan contexto local:\n",
        "\n",
        "  “não gosto” ≠ “gosto”\n",
        "  \n",
        "  “muito bom” → sentimiento positivo que una sola palabra no revela."
      ],
      "metadata": {
        "id": "yNajtNcdnzFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador con n-gramas (de 1 a 3 palabras)\n",
        "c_vect = CountVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "# Representamos los documentos en una matriz documento × término\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Mostramos información sobre la matriz generada\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "print(\"Número de documentos:\", p_counts.shape[0])\n",
        "print(\"Tamanho do vocabulário (features):\", p_counts.shape[1])\n",
        "\n",
        "# Mostramos algunas features detectadas\n",
        "print(\"\\nExemplos de features (n-gramas detectados):\")\n",
        "print(list(c_vect.get_feature_names_out())[:30])  # solo los 30 primeros\n",
        "\n",
        "# COMENTARIO: logicamente ahora vemos mas vocabulario que antes porque ahora al tener n-gramas tenemos mas combinaicones de palabras nuevas"
      ],
      "metadata": {
        "id": "FPJhAOWFoYEU",
        "outputId": "0be32ba0-7dec-45b8-e1df-0c16ab87bc4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 17120)\n",
            "Número de documentos: 1617\n",
            "Tamanho do vocabulário (features): 17120\n",
            "\n",
            "Exemplos de features (n-gramas detectados):\n",
            "['abafadiço', 'abafadiço sai', 'abafadiço sai abelha', 'abaixa', 'abaixa rabo', 'abaixa rabo se', 'abaixo', 'abaixo come', 'abandona', 'abarca', 'abarca pouco', 'abarca pouco abraça', 'abastado', 'abastado quem', 'abastado quem dá', 'abegão', 'abelha', 'abelha do', 'abelha do cortiço', 'abelha não', 'abelha não leva', 'abelha senão', 'abelha senão quem', 'abelhas', 'abelhas para', 'abelhas para as', 'abençoada', 'aberta', 'aberta ou', 'aberta ou entra']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Exemplo 1.4 – Ajustes avançados em CountVectorizer"
      ],
      "metadata": {
        "id": "fj2-yRnYY2Pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_features:\tDefine un número máximo de palabras a conservar. Usa las más frecuentes.\t  max_features=1000 mantiene solo las 1000 palabras más comunes.\n",
        "\n",
        "max_df:\tPalabras que aparecen en demasiados documentos se descartan. Puede ser valor absoluto o proporción.\t max_df=0.75 ignora palabras que aparecen en ≥75 % de los documentos.\n",
        "\n",
        "min_df:\tPalabras que aparecen muy poco también se descartan.\tmin_df=5 mantiene solo palabras que aparecen en ≥5 documentos.\n",
        "\n",
        "strip_accents:\tElimina acentos antes del análisis.\t\"coração\" → \"coracao\"\n",
        "\n",
        "stop_words:\tPermite eliminar stopwords (palabras vacías sin significado, como “o”, “de”, “em”).\tSe puede usar la lista de stopwords del idioma portugués en NLTK."
      ],
      "metadata": {
        "id": "Au7ZeZNupkRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "\n",
        "# Descargamos las stopwords de NLTK (solo la primera vez)\n",
        "nltk.download('stopwords')\n",
        "pt_stop = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador con parámetros personalizados\n",
        "c_vect = CountVectorizer(\n",
        "    ngram_range=(1, 3),       # unigramas, bigramas y trigramas\n",
        "    strip_accents='unicode',  # elimina acentos\n",
        "    stop_words=pt_stop,       # usa stopwords en portugués\n",
        "    min_df=5,                 # ignora palabras que aparecen en <5 documentos\n",
        "    max_df=0.75,              # ignora palabras que aparecen en >75% de documentos\n",
        "    max_features=1000         # mantiene las 1000 más frecuentes\n",
        ")\n",
        "\n",
        "# Generamos la matriz documento × término\n",
        "p_counts = c_vect.fit_transform(docs)\n",
        "\n",
        "# Información sobre la matriz\n",
        "print(\"Forma da matriz:\", p_counts.shape)\n",
        "print(\"Número de features finais:\", len(c_vect.get_feature_names_out()))\n",
        "\n",
        "# Visualizamos algunas features seleccionadas\n",
        "print(\"\\nAlgumas features seleccionadas:\")\n",
        "print(list(c_vect.get_feature_names_out())[:30])\n"
      ],
      "metadata": {
        "id": "K5-IldQiqc_w",
        "outputId": "69cdb43c-c55d-4527-a8d6-848b34eeca94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 314)\n",
            "Número de features finais: 314\n",
            "\n",
            "Algumas features seleccionadas:\n",
            "['abril', 'agua', 'aguas', 'ajuda', 'alcanca', 'alegria', 'alto', 'amanha', 'amigo', 'amigos', 'amizade', 'amor', 'amores', 'anda', 'andar', 'ano', 'antes', 'apanha', 'aprende', 'arvore', 'assim', 'ate', 'baixo', 'barriga', 'basta', 'bebe', 'beleza', 'bem', 'bem nao', 'boa']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ate', 'eramos', 'estao', 'estavamos', 'estiveramos', 'estivessemos', 'foramos', 'fossemos', 'ha', 'hao', 'houveramos', 'houverao', 'houveriamos', 'houvessemos', 'ja', 'nao', 'sao', 'sera', 'serao', 'seriamos', 'so', 'tambem', 'tera', 'terao', 'teriamos', 'tinhamos', 'tiveramos', 'tivessemos', 'voce', 'voces'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Exemplo 1.5 – Representação TF-IDF com TfidfVectorizer"
      ],
      "metadata": {
        "id": "4Kc6-uH_ZPwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "Cambiar el modelo de representación de texto:\n",
        "👉 pasar de contar palabras (Bag-of-Words) a ponderarlas según su importancia (TF-IDF).\n",
        "\n",
        "Esto mejora el rendimiento de muchos modelos, ya que no todas las palabras tienen el mismo peso en la comprensión de un texto.\n",
        "\n",
        "🧠 Explicación teórica\n",
        "\n",
        "El TF-IDF combina dos ideas:\n",
        "\n",
        "TF (Term Frequency) → mide cuántas veces aparece una palabra en un documento.\n",
        "\n",
        "Cuanto más aparece, más relevante es para ese documento.\n",
        "\n",
        "Ejemplo: si en un proverbio la palabra “vida” aparece 3 veces, TF(vida)=3.\n",
        "\n",
        "IDF (Inverse Document Frequency) → mide en cuántos documentos aparece esa palabra.\n",
        "\n",
        "Cuanto más común sea en todos los textos, menos informativa es.\n",
        "\n",
        "Ejemplo: si “vida” aparece en 90 % de los proverbios, su IDF será pequeño.\n",
        "\n",
        "💬 Interpretación\n",
        "\n",
        "A diferencia de CountVectorizer, aquí los valores no son enteros, sino ponderaciones decimales (entre 0 y 1).\n",
        "\n",
        "Palabras muy frecuentes en un documento, pero poco comunes en el corpus total, tienen un peso TF-IDF alto.\n",
        "\n",
        "Las palabras comunes (como “de”, “o”, “a”) tienen un peso bajo o nulo.\n",
        "\n",
        "📈 Así, la representación TF-IDF resalta las palabras más características de cada texto."
      ],
      "metadata": {
        "id": "BYfcYn9QrIy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Exemplo 1.5 – Representação TF-IDF com *TfidfVectorizer*\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Objetivo\n",
        "Cambiar el modelo de representación de texto:\n",
        "\n",
        "👉 Pasar de **contar palabras (Bag-of-Words)** a **ponderarlas según su importancia (TF-IDF)**.  \n",
        "\n",
        "Esto mejora el rendimiento de muchos modelos, ya que no todas las palabras tienen el mismo peso en la comprensión de un texto.\n",
        "\n",
        "---\n",
        "\n",
        "### 💡 Explicación teórica\n",
        "\n",
        "El **TF-IDF** combina dos ideas fundamentales:\n",
        "\n",
        "#### 🔸 TF (*Term Frequency*)\n",
        "Mide cuántas veces aparece una palabra en un documento.  \n",
        "Cuanto más aparece, más relevante es para ese documento.\n",
        "\n",
        "📘 Ejemplo:  \n",
        "Si en un proverbio la palabra *vida* aparece 3 veces,  \n",
        "entonces TF(vida) = 3.\n",
        "\n",
        "#### 🔸 IDF (*Inverse Document Frequency*)\n",
        "Mide en cuántos documentos aparece esa palabra.  \n",
        "Cuanto más común sea en todos los textos, menos informativa es.\n",
        "\n",
        "📘 Ejemplo:  \n",
        "Si *vida* aparece en 90 % de los proverbios, su IDF será pequeño.\n",
        "\n",
        "---\n",
        "\n",
        "### 📈 Interpretación\n",
        "\n",
        "A diferencia de **CountVectorizer**, aquí los valores no son enteros,  \n",
        "sino **ponderaciones decimales** (entre 0 y 1).  \n",
        "\n",
        "- Palabras muy frecuentes en un documento, pero poco comunes en el corpus total,  \n",
        "  → tienen un peso **TF-IDF alto**.  \n",
        "- Palabras comunes (como *de*, *o*, *a*)  \n",
        "  → tienen un peso **bajo o nulo**.  \n",
        "\n",
        "🧾 Así, la representación **TF-IDF resalta las palabras más características** de cada texto.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧮 Fórmula del cálculo TF-IDF\n",
        "\n",
        "El valor **TF-IDF** de un término $t$ en un documento $d$ dentro de un corpus de $N$ documentos se calcula como:\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\log \\left( \\frac{N}{\\text{DF}(t)} \\right)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $\\text{TF}(t, d)$ → frecuencia del término *t* en el documento *d*  \n",
        "- $\\text{DF}(t)$ → número de documentos que contienen el término *t*  \n",
        "- $N$ → número total de documentos del corpus"
      ],
      "metadata": {
        "id": "xUBR6aChtATF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargamos los proverbios\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos el vectorizador TF-IDF\n",
        "tf_vect = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),  # unigramas y bigramas\n",
        "    min_df=3,            # debe aparecer en al menos 3 documentos\n",
        "    max_df=0.5,          # y en no más del 50% de los documentos\n",
        "    max_features=500     # máximo de 500 features\n",
        ")\n",
        "\n",
        "# Generamos la matriz TF-IDF\n",
        "p_tf = tf_vect.fit_transform(docs)\n",
        "\n",
        "# Mostramos resultados básicos\n",
        "print(\"Forma da matriz:\", p_tf.shape)\n",
        "print(\"Número de features:\", len(tf_vect.get_feature_names_out()))\n",
        "\n",
        "# Mostramos algunas features y valores TF-IDF del primer documento\n",
        "print(\"\\nFeatures (palabras o n-gramas):\")\n",
        "print(list(tf_vect.get_feature_names_out())[:20])\n",
        "\n",
        "print(\"\\nRepresentação do primeiro documento (valores TF-IDF):\")\n",
        "print(p_tf[0, :].toarray().tolist())\n"
      ],
      "metadata": {
        "id": "dDC__DCntSY3",
        "outputId": "4a9a3094-b05d-43c5-b2c7-5839eb4ecff6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz: (1617, 500)\n",
            "Número de features: 500\n",
            "\n",
            "Features (palabras o n-gramas):\n",
            "['abril', 'acaba', 'ainda', 'ajuda', 'alcança', 'alegria', 'alheia', 'alheio', 'alma', 'alto', 'ama', 'amanhã', 'ambição', 'amigo', 'amigos', 'amizade', 'amor', 'amores', 'anda', 'andar']\n",
            "\n",
            "Representação do primeiro documento (valores TF-IDF):\n",
            "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9382731425466352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34589522976713843, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Exemplo 1.6 – Reutilizar um Vectorizer com novos documentos"
      ],
      "metadata": {
        "id": "1ET0SEztZ9Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "Aprender a representar nuevos documentos (no vistos durante el entrenamiento) usando el mismo vectorizador ya entrenado.\n",
        "Esto es fundamental cuando trabajas con modelos de aprendizaje automático, ya que:\n",
        "\n",
        "⚠️ El vocabulario del modelo debe mantenerse constante.\n",
        "Si reentrenas el vectorizador con nuevos textos, el vocabulario puede cambiar,\n",
        "y los vectores dejarían de ser comparables.\n",
        "\n",
        "🧠 Explicación teórica\n",
        "\n",
        "Cuando entrenamos un CountVectorizer o TfidfVectorizer, ocurren dos pasos:\n",
        "\n",
        "Método\tQué hace\n",
        "fit_transform(docs)\tAprende el vocabulario del corpus y transforma los documentos en vectores. (Se usa con los datos de entrenamiento).\n",
        "transform(new_docs)\tUsa el vocabulario aprendido previamente para transformar nuevos documentos.\n",
        "\n",
        "💡 Es decir:\n",
        "\n",
        "fit_transform() se usa una sola vez durante el entrenamiento.\n",
        "\n",
        "transform() se usa cada vez que quieres representar nuevos textos con el mismo modelo"
      ],
      "metadata": {
        "id": "bPGPZhBXtk5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Cargamos los proverbios (entrenamiento)\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# Creamos y entrenamos el vectorizador\n",
        "tf_vect = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),   # unigramas y bigramas\n",
        "    min_df=3,             # debe aparecer en al menos 3 documentos\n",
        "    max_df=0.5,           # no más del 50% de los documentos\n",
        "    max_features=500      # máximo de 500 palabras o n-gramas\n",
        ")\n",
        "p_tf = tf_vect.fit_transform(docs)\n",
        "\n",
        "# Nuevos documentos (no vistos durante el entrenamiento)\n",
        "novos_docs = [\n",
        "    \"amor é fogo que arde sem se ver\",\n",
        "    \"verdes são os campos\"\n",
        "]\n",
        "\n",
        "# Transformamos los nuevos documentos con el mismo vectorizador\n",
        "novos_vects = tf_vect.transform(novos_docs)\n",
        "\n",
        "# Exploramos la matriz resultante\n",
        "print(\"Forma da matriz dos novos documentos:\", novos_vects.shape)\n",
        "print(\"\\nRepresentação TF-IDF dos novos documentos:\")\n",
        "print(novos_vects.toarray())\n",
        "\n",
        "\n",
        "#🧾 Explicación\n",
        "  # tf_vect.fit_transform(docs) → se usa solo con los proverbios iniciales (entrenamiento).\n",
        "  # tf_vect.transform(novos_docs) → aplica el mismo vocabulario al nuevo texto.\n",
        "  # Palabras que no estaban en el vocabulario original serán ignoradas.\n",
        "\n",
        "# Output es un array (2, 500) donde solo hay decimales en aquellas posiciones de las palbras del documento que marcan su importancia (de 0 a 1) y 0 en el resto, las palabras que no esten en el modelo del vocubulario\n",
        "# inicial entonces serán obviadas\n"
      ],
      "metadata": {
        "id": "XIh9nO6HuD3Q",
        "outputId": "09fc9aee-963c-45a3-f849-52ccdf3bf67c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz dos novos documentos: (2, 500)\n",
            "\n",
            "Representação TF-IDF dos novos documentos:\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.46215083 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.50949558\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.21335991 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.23497398 0.\n",
            "  0.         0.         0.         0.         0.         0.38080138\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.53017539 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.64998539 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.7599467\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 1.3 Desafio – Similaridade do Cosseno entre documentos"
      ],
      "metadata": {
        "id": "RhdQyMwlawWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 Objetivo\n",
        "\n",
        "El desafío propone que crees una función en Python que, dada una colección de documentos y un nuevo documento, encuentre los n documentos más parecidos según la similaridad del coseno.\n",
        "\n",
        "Esto te permitirá medir qué tan similares son dos textos basándote en su representación numérica (por ejemplo, TF-IDF).\n",
        "\n",
        "🧠 Explicación teórica\n",
        "\n",
        "Cuando representamos los documentos con TF-IDF, cada documento se convierte en un vector de características numéricas.\n",
        "Para medir la semejanza entre dos documentos, usamos la similaridad del coseno (cosine similarity)."
      ],
      "metadata": {
        "id": "qojDSanKw65O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPCION 1 (CHATI)"
      ],
      "metadata": {
        "id": "vauYf5Luzniw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1️⃣ Cargar los documentos base\n",
        "docs = carrega_docs(\"proverbios.txt\")\n",
        "\n",
        "# 2️⃣ Crear y entrenar el vectorizador TF-IDF\n",
        "tf_vect = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.5)\n",
        "tf_matrix = tf_vect.fit_transform(docs)\n",
        "\n",
        "# 3️⃣ Definir la función que devuelve los n documentos más similares\n",
        "def documentos_parecidos(tf_vect, tf_matrix, novo_doc, n=3):\n",
        "    \"\"\"\n",
        "    Dado un vectorizador TF-IDF entrenado, su matriz y un nuevo documento,\n",
        "    devuelve los n documentos más parecidos según la similaridad del coseno.\n",
        "    \"\"\"\n",
        "    # Transformar el nuevo documento al mismo espacio vectorial\n",
        "    novo_vec = tf_vect.transform([novo_doc])\n",
        "\n",
        "    # Calcular la similaridad del coseno con todos los documentos\n",
        "    similaridades = cosine_similarity(novo_vec, tf_matrix)[0]\n",
        "\n",
        "    # Obtener los índices de los documentos más similares\n",
        "    indices = np.argsort(similaridades)[::-1][:n]\n",
        "\n",
        "    # Devolver los documentos y sus puntuaciones\n",
        "    resultados = [(docs[i], similaridades[i]) for i in indices]\n",
        "    return resultados\n",
        "\n",
        "# 4️⃣ Probar la función con un nuevo texto\n",
        "novo_texto = \"quem espera sempre alcança\"\n",
        "resultados = documentos_parecidos(tf_vect, tf_matrix, novo_texto, n=5)\n",
        "\n",
        "print(\"🔎 Documentos mais semelhantes a:\", novo_texto)\n",
        "for doc, sim in resultados:\n",
        "    print(f\"{sim:.3f} → {doc}\")\n"
      ],
      "metadata": {
        "id": "QJ0HPoyha02y",
        "outputId": "7d755ae8-5a5c-42c8-d009-80cbc26c1c89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Documentos mais semelhantes a: quem espera sempre alcança\n",
            "1.000 → quem espera sempre alcança.\n",
            "0.643 → quem porfia sempre alcança.\n",
            "0.577 → quem tem esperança sempre alcança.\n",
            "0.547 → quem espera desespera.\n",
            "0.420 → quem espera por sapatos de defunto, toda a vida anda descalço.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPCION 2 (CLASE)"
      ],
      "metadata": {
        "id": "0v4tYgn5zqTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar(docs, doc, n=10, smin=0.5):\n",
        "    tf_vect = TfidfVectorizer(ngram_range=(1,3), strip_accents='unicode',\n",
        "                              max_features=500, min_df=5, max_df=0.75)\n",
        "\n",
        "    vects = tf_vect.fit_transform(docs)\n",
        "    doc_vec = tf_vect.transform([doc])\n",
        "\n",
        "    vocabulary = tf_vect.get_feature_names_out()\n",
        "    for i in doc_vec.indices:\n",
        "        print(i, vocabulary[i])\n",
        "\n",
        "    # cálculo da similaridade\n",
        "    cosine_matrix = cosine_similarity(vects, doc_vec)\n",
        "    print(cosine_matrix.shape)     # Genera una matriz de tamaño (n_docs, 1) con la similitud entre cada documento del corpus y el nuevo documento.\n",
        "\n",
        "    lista_cos = [(docs[i], e[0]) for i, e in enumerate(cosine_matrix)]\n",
        "    lista_cos.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [(d, c) for d, c in lista_cos[:n] if c > smin]\n",
        "\n",
        "doc = 'Em terra de cegos, quem tem um olho é rei.'\n",
        "similar = most_similar(docs, doc)\n",
        "print(similar)"
      ],
      "metadata": {
        "id": "zcz8xjXIzs90",
        "outputId": "64dc7530-b811-4cea-8c35-3c07f96600c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85 de\n",
            "114 em\n",
            "116 em terra\n",
            "262 olho\n",
            "319 quem\n",
            "331 quem tem\n",
            "337 rei\n",
            "383 tem\n",
            "390 terra\n",
            "391 terra de\n",
            "406 um\n",
            "(1617, 1)\n",
            "[('em terra de cegos, quem tem um olho é rei.', np.float64(1.0)), ('em terra de sapos, de cócoras como eles.', np.float64(0.6338665651998857)), ('em terra de saci, calça comprida dá pra dois.', np.float64(0.6058372801802083)), ('em terra de sapo, mosquito não dá rasante.', np.float64(0.5772407093557776))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2º PARTE DE LA PRÁCTICA: Classificação de texto com Scikit-Learn\n"
      ],
      "metadata": {
        "id": "3OSkNKoSNkW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo es crear o cargar un conjunto de documentos de texto que tengan categorías (clases) asociadas, y preparar los datos para entrenar un modelo de clasificación de texto.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "publico.txt → contiene títulos de noticias normales.\n",
        "\n",
        "ipublico.txt → contiene títulos humorísticos del suplemento Inimigo Público.\n",
        "\n",
        "Cada línea del archivo representa un documento (un texto corto)."
      ],
      "metadata": {
        "id": "_LtHzCFsNsMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/TitulosPublico_N.txt -O publico.txt\n",
        "!wget https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/inimigopublico_H.txt -O ipublico.txt\n"
      ],
      "metadata": {
        "id": "Rxs0XU5aNygc",
        "outputId": "43a1e8bc-970c-49be-ce95-b2ab9e676d91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-07 10:28:52--  https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/TitulosPublico_N.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 159919 (156K) [text/plain]\n",
            "Saving to: ‘publico.txt’\n",
            "\n",
            "\rpublico.txt           0%[                    ]       0  --.-KB/s               \rpublico.txt         100%[===================>] 156.17K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-10-07 10:28:52 (77.7 MB/s) - ‘publico.txt’ saved [159919/159919]\n",
            "\n",
            "--2025-10-07 10:28:52--  https://raw.githubusercontent.com/NLP-CISUC/Recognizing-Humor-in-Portuguese/master/Datasets/Originais/inimigopublico_H.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 222678 (217K) [text/plain]\n",
            "Saving to: ‘ipublico.txt’\n",
            "\n",
            "ipublico.txt        100%[===================>] 217.46K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2025-10-07 10:28:52 (60.8 MB/s) - ‘ipublico.txt’ saved [222678/222678]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 Objetivo\n",
        "\n",
        "El propósito de este ejemplo es:\n",
        "\n",
        "Cargar los documentos de texto desde los ficheros descargados (publico.txt y ipublico.txt).\n",
        "\n",
        "Crear una lista de etiquetas (clases) para cada documento:\n",
        "\n",
        "'P' → título del Público\n",
        "\n",
        "'IP' → título del Inimigo Público"
      ],
      "metadata": {
        "id": "8b2c7uA3OG0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Exemplo 1.7 - Cargar documentos y clases\n",
        "# =====================================================\n",
        "\n",
        "# Función para cargar documentos desde un fichero de texto\n",
        "coment = '#'\n",
        "\n",
        "def carrega_docs(ficheiro):\n",
        "    docs = []\n",
        "    with open(ficheiro, encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for txt in lines:\n",
        "            txt = txt.strip()\n",
        "            if len(txt) > 0 and txt[0] != coment:\n",
        "                docs.append(txt)\n",
        "    return docs\n",
        "\n",
        "# Cargar los títulos del Público e Inimigo Público\n",
        "docs_p = carrega_docs('publico.txt')\n",
        "docs_ip = carrega_docs('ipublico.txt')\n",
        "\n",
        "# Crear lista de textos (X) y sus etiquetas (y)\n",
        "lista_X = docs_p + docs_ip\n",
        "lista_y = ['P' if i < len(docs_p) else 'IP' for i in range(len(lista_X))]\n",
        "\n",
        "# Comprobación\n",
        "print(f\"Número total de documentos: {len(lista_X)}\")\n",
        "print(f\"Número de etiquetas: {len(lista_y)}\")\n",
        "print(f\"Ejemplo documento: {lista_X[0]}\")\n",
        "print(f\"Clase asociada: {lista_y[0]}\")"
      ],
      "metadata": {
        "id": "yz0CsHbGOMUn",
        "outputId": "3e05b2fc-9566-40e4-fea3-afe86f82bf34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de documentos: 4480\n",
            "Número de etiquetas: 4480\n",
            "Ejemplo documento: Os maiores escândalos de abuso sexual na Igreja desde a eleição do Papa Francisco.\n",
            "Clase asociada: P\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entonces estamos juntando todo en un mismo archivo pero cada documento tiene una etiqueta segun si proceda de publico.txt o ipublico.txt"
      ],
      "metadata": {
        "id": "KkzQy_vwO2Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧩 Exemplo 1.8 — Divisão treino/teste"
      ],
      "metadata": {
        "id": "4ARpD4uVO-PL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de este paso es dividir nuestro conjunto de datos (lista_X y lista_y) en dos partes:\n",
        "\n",
        "Conjunto de entrenamiento (train):\n",
        "Usado para entrenar el modelo (ajustar sus parámetros).\n",
        "\n",
        "Conjunto de prueba (test):\n",
        "Usado para evaluar el rendimiento del modelo con datos nuevos, es decir, textos que el modelo no ha visto durante el entrenamiento"
      ],
      "metadata": {
        "id": "ZBxy9CbtPClO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Exemplo 1.8 - Separar dados em treino e teste\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividir os dados em treino (70%) e teste (30%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    lista_X, lista_y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"Total de documentos: {len(lista_X)}\")\n",
        "print(f\"Treino: {len(X_train)} documentos\")\n",
        "print(f\"Teste:  {len(X_test)} documentos\")\n",
        "\n",
        "# Ejemplo de un título y su clase\n",
        "print(\"\\nExemplo de treino:\")\n",
        "print(X_train[0])\n",
        "print(f\"Classe: {y_train[0]}\")"
      ],
      "metadata": {
        "id": "-2gz3dgcO-9u",
        "outputId": "439f36cc-b079-46eb-879e-551dda72e643",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de documentos: 4480\n",
            "Treino: 3136 documentos\n",
            "Teste:  1344 documentos\n",
            "\n",
            "Exemplo de treino:\n",
            "Quantos mais super-heróis cabem no guarda-chuva das séries?.\n",
            "Classe: P\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧩 Exemplo 1.9 — Representación de los documentos"
      ],
      "metadata": {
        "id": "35fbhBIgPJkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 Objetivo\n",
        "\n",
        "Transformar nuestros textos (strings) en vectores numéricos, que es la única forma en que los algoritmos de Machine Learning pueden trabajar.\n",
        "\n",
        "⚙️ Recordatorio\n",
        "\n",
        "Hasta ahora tenemos:\n",
        "\n",
        "X_train, X_test → los textos.\n",
        "\n",
        "y_train, y_test → las etiquetas (‘P’ o ‘IP’).\n",
        "\n",
        "Ahora debemos convertir los textos en números usando un vectorizador."
      ],
      "metadata": {
        "id": "NqZfLqpuPhkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Exemplo 1.9 - Representação de texto com CountVectorizer\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Crear el vectorizador\n",
        "count_vect = CountVectorizer()\n",
        "\n",
        "# Ajustar el vectorizador al conjunto de treino\n",
        "count_vects_train = count_vect.fit_transform(X_train)\n",
        "\n",
        "# Transformar el conjunto de teste usando el mismo vocabulario\n",
        "count_vects_test = count_vect.transform(X_test)\n",
        "\n",
        "# Mostrar información\n",
        "print(\"Forma da matriz de treino:\", count_vects_train.shape)\n",
        "print(\"Forma da matriz de teste:\", count_vects_test.shape)\n",
        "\n",
        "# Ver algunas features (palabras del vocabulario)\n",
        "print(\"\\nExemplo de features extraídas:\")\n",
        "print(count_vect.get_feature_names_out()[:20])\n"
      ],
      "metadata": {
        "id": "bWoTCKhhPfU_",
        "outputId": "838c4286-39a2-430d-d622-e938327cdca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma da matriz de treino: (3136, 8870)\n",
            "Forma da matriz de teste: (1344, 8870)\n",
            "\n",
            "Exemplo de features extraídas:\n",
            "['000' '01' '10' '100' '103' '107' '10yearchallenge' '11' '110' '1111'\n",
            " '112' '12' '13' '130' '1300' '132' '134' '14' '140' '141']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧩 Exemplo 1.10 — Entrenamiento de un clasificador Naive Bayes\n",
        "🧠 Objetivo\n",
        "\n",
        "Usar los vectores numéricos que creamos con CountVectorizer para entrenar un modelo de clasificación de texto.\n",
        "En este caso se usa el Naive Bayes Multinomial, un algoritmo clásico y muy eficiente para tareas de clasificación de texto (como detección de spam, análisis de sentimientos, o detección de humor).\n",
        "\n",
        "⚙️ ¿Por qué Naive Bayes?\n",
        "En pocas palabras: estima la probabilidad de que un texto pertenezca a una clase basándose en las palabras que contiene."
      ],
      "metadata": {
        "id": "mWAvqkNsPkZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Exemplo 1.10 - Treinar um classificador Naive Bayes\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Crear y entrenar el clasificador\n",
        "clf = MultinomialNB().fit(count_vects_train, y_train)\n",
        "\n",
        "# Predecir las clases del conjunto de teste\n",
        "predicted = clf.predict(count_vects_test)\n",
        "\n",
        "# Mostrar algunas predicciones\n",
        "for doc, category, true_label in zip(X_test[:5], predicted[:5], y_test[:5]):\n",
        "    print(f\"{doc!r} => Predicho: {category} | Real: {true_label}\")\n"
      ],
      "metadata": {
        "id": "NxYbFJ4zP0mP",
        "outputId": "8c741677-471a-4594-fb6d-682ddca40bff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Professores doentes estão a ser obrigados a regressar às escolas.' => Predicho: P | Real: P\n",
            "'Prendas de ofereça um banco português à avó' => Predicho: IP | Real: IP\n",
            "'Tribunal de recurso mantém hacker Rui Pinto em prisão domiciliária.' => Predicho: P | Real: P\n",
            "'Ministério Público: “O problema é o controlo sindical, não o controlo político”.' => Predicho: P | Real: P\n",
            "'Quando as europeias foram um \"trinta e um\" para o PS.' => Predicho: P | Real: P\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧩 Exemplo 1.11 — Avaliação do classificador\n",
        "🧠 Objetivo\n",
        "\n",
        "El objetivo de este paso es evaluar cuantitativamente cómo de bien está funcionando nuestro modelo.\n",
        "Para ello usaremos las métricas más comunes en clasificación:"
      ],
      "metadata": {
        "id": "vO-sCl63P5OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📘 Explicación rápida\n",
        "\n",
        "metrics.classification_report(y_test, predicted, target_names=set(lista_y))\n",
        "→ muestra un resumen de precisión, exhaustividad y F1 para cada clase (‘P’ e ‘IP’).\n",
        "\n",
        "metrics.confusion_matrix(y_test, predicted)\n",
        "→ muestra cuántas predicciones fueron correctas o erróneas para cada clase."
      ],
      "metadata": {
        "id": "NaamUmZeQDGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Exemplo 1.11 - Avaliação do classificador\n",
        "# =====================================================\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "# Relatório de métricas detalhado\n",
        "print(metrics.classification_report(\n",
        "    y_test, predicted, target_names=set(lista_y))\n",
        ")\n",
        "\n",
        "# Matriz de confusión\n",
        "print(\"Matriz de confusão:\")\n",
        "print(metrics.confusion_matrix(y_test, predicted))\n"
      ],
      "metadata": {
        "id": "ubjETkq3QDzk",
        "outputId": "e5d6a852-2372-4b0a-8a11-b8f8a3622f3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           P       0.74      0.89      0.81       667\n",
            "          IP       0.86      0.69      0.76       677\n",
            "\n",
            "    accuracy                           0.79      1344\n",
            "   macro avg       0.80      0.79      0.79      1344\n",
            "weighted avg       0.80      0.79      0.79      1344\n",
            "\n",
            "Matriz de confusão:\n",
            "[[593  74]\n",
            " [212 465]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# La parte de support te dice la cantidad de muestras para cada clase --> nos puede servir para saber si las clases estan balanceadas o no"
      ],
      "metadata": {
        "id": "Kz7NdS2jUP3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧩 1.6 Exercícios — Objetivo general\n",
        "\n",
        "El objetivo de esta parte es experimentar y comparar distintos:\n",
        "\n",
        "Vectorizadores (CountVectorizer, TfidfVectorizer)\n",
        "\n",
        "Modelos clasificadores (Naive Bayes, SVM, Logistic Regression, etc.)\n",
        "\n",
        "Parámetros de configuración (n-gramas, min_df, max_df, etc.)\n",
        "\n",
        "👉 La meta es entender cómo cambian los resultados (accuracy, precision, recall, f1) al modificar las representaciones o los clasificadores."
      ],
      "metadata": {
        "id": "iRDR67oySvkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 Exercício 1.2 — Experimentar con el vectorizador\n",
        "🎯 Qué pide\n",
        "\n",
        "Analizar el impacto de cambiar los parámetros del vectorizador:\n",
        "\n",
        "ngram_range → cambia el tamaño de las secuencias de palabras.\n",
        "\n",
        "min_df → ignora palabras que aparecen en pocos documentos.\n",
        "\n",
        "max_df → ignora palabras que aparecen en casi todos los documentos.\n",
        "\n",
        "max_features → limita el número de palabras del vocabulario.\n",
        "\n",
        "También te pide probar otro tipo de vectorizador: el TfidfVectorizer."
      ],
      "metadata": {
        "id": "CjqHAV5OSykv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# Crear un vectorizador TF-IDF con diferentes parámetros\n",
        "tfidf_vect = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),   # usa unigramas y bigramas\n",
        "    min_df=3,             # palabra debe aparecer al menos en 3 documentos\n",
        "    max_df=0.7,           # ignora palabras muy frecuentes\n",
        "    max_features=1000     # limita el vocabulario\n",
        ")\n",
        "\n",
        "# Transformar los textos\n",
        "tfidf_train = tfidf_vect.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vect.transform(X_test)\n",
        "\n",
        "# Entrenar y evaluar\n",
        "clf = MultinomialNB().fit(tfidf_train, y_train)\n",
        "pred = clf.predict(tfidf_test)\n",
        "\n",
        "print(metrics.classification_report(y_test, pred))"
      ],
      "metadata": {
        "id": "S62A3jUqS1Ng",
        "outputId": "701cbf58-993d-4116-9a8b-8087e2427163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.76      0.73      0.74       667\n",
            "           P       0.74      0.77      0.76       677\n",
            "\n",
            "    accuracy                           0.75      1344\n",
            "   macro avg       0.75      0.75      0.75      1344\n",
            "weighted avg       0.75      0.75      0.75      1344\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Ejercicio 1.2 - Comparando CountVectorizer con distintos parámetros\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# 1️⃣ Vectorizador con unigramas (solo palabras individuales)\n",
        "cv1 = CountVectorizer(ngram_range=(1,1))\n",
        "X_train_cv1 = cv1.fit_transform(X_train)\n",
        "X_test_cv1 = cv1.transform(X_test)\n",
        "\n",
        "clf1 = MultinomialNB().fit(X_train_cv1, y_train)\n",
        "pred1 = clf1.predict(X_test_cv1)\n",
        "print(\"=== Modelo 1: Unigramas ===\")\n",
        "print(metrics.classification_report(y_test, pred1))\n",
        "\n",
        "# 2️⃣ Vectorizador con unigramas y bigramas (considera pares de palabras)\n",
        "cv2 = CountVectorizer(ngram_range=(1,2), max_features=2000, max_df=0.75, min_df=3)\n",
        "X_train_cv2 = cv2.fit_transform(X_train)\n",
        "X_test_cv2 = cv2.transform(X_test)\n",
        "\n",
        "clf2 = MultinomialNB().fit(X_train_cv2, y_train)\n",
        "pred2 = clf2.predict(X_test_cv2)\n",
        "print(\"\\n=== Modelo 2: Unigramas + Bigramas ===\")\n",
        "print(metrics.classification_report(y_test, pred2))\n",
        "\n",
        "# 3️⃣ Vectorizador con limpieza adicional (sin acentos y stopwords en portugués)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "pt_stop = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "cv3 = CountVectorizer(\n",
        "    ngram_range=(1,2),\n",
        "    strip_accents='unicode',\n",
        "    stop_words=pt_stop,\n",
        "    min_df=5,\n",
        "    max_df=0.7,\n",
        "    max_features=1500\n",
        ")\n",
        "\n",
        "X_train_cv3 = cv3.fit_transform(X_train)\n",
        "X_test_cv3 = cv3.transform(X_test)\n",
        "\n",
        "clf3 = MultinomialNB().fit(X_train_cv3, y_train)\n",
        "pred3 = clf3.predict(X_test_cv3)\n",
        "print(\"\\n=== Modelo 3: Bigramas + Stopwords removidas + Acentos normalizados ===\")\n",
        "print(metrics.classification_report(y_test, pred3))"
      ],
      "metadata": {
        "id": "V87GFbFsT-4C",
        "outputId": "728a13d8-ee4a-428b-cc73-66b8b0f37256",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Modelo 1: Unigramas ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.74      0.89      0.81       667\n",
            "           P       0.86      0.69      0.76       677\n",
            "\n",
            "    accuracy                           0.79      1344\n",
            "   macro avg       0.80      0.79      0.79      1344\n",
            "weighted avg       0.80      0.79      0.79      1344\n",
            "\n",
            "\n",
            "=== Modelo 2: Unigramas + Bigramas ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.77      0.75      0.76       667\n",
            "           P       0.76      0.78      0.77       677\n",
            "\n",
            "    accuracy                           0.77      1344\n",
            "   macro avg       0.77      0.77      0.77      1344\n",
            "weighted avg       0.77      0.77      0.77      1344\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ate', 'eramos', 'estao', 'estavamos', 'estiveramos', 'estivessemos', 'foramos', 'fossemos', 'ha', 'hao', 'houveramos', 'houverao', 'houveriamos', 'houvessemos', 'ja', 'nao', 'sao', 'sera', 'serao', 'seriamos', 'so', 'tambem', 'tera', 'terao', 'teriamos', 'tinhamos', 'tiveramos', 'tivessemos', 'voce', 'voces'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Modelo 3: Bigramas + Stopwords removidas + Acentos normalizados ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.76      0.77      0.76       667\n",
            "           P       0.77      0.76      0.77       677\n",
            "\n",
            "    accuracy                           0.77      1344\n",
            "   macro avg       0.77      0.77      0.77      1344\n",
            "weighted avg       0.77      0.77      0.77      1344\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 Exercício 1.3 — Probar distintos clasificadores\n",
        "🎯 Qué pide\n",
        "\n",
        "Entrenar y comparar otros modelos, cambiando solo el clasificador, mientras mantienes la misma representación vectorial.\n",
        "\n",
        "Modelos sugeridos:\n",
        "\n",
        "sklearn.linear_model.LogisticRegression\n",
        "\n",
        "sklearn.neighbors.KNeighborsClassifier\n",
        "\n",
        "sklearn.svm.SVC (máquinas de soporte vectorial)\n",
        "\n",
        "sklearn.ensemble.RandomForestClassifier"
      ],
      "metadata": {
        "id": "My1AcLaaS2I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "# Usar el mismo vectorizador que antes\n",
        "X_train_vec = count_vects_train\n",
        "X_test_vec = count_vects_test\n",
        "\n",
        "# 1️⃣ Logistic Regression\n",
        "logreg = LogisticRegression(max_iter=1000).fit(X_train_vec, y_train)\n",
        "pred_log = logreg.predict(X_test_vec)\n",
        "print(\"Logistic Regression:\\n\", metrics.classification_report(y_test, pred_log))\n",
        "\n",
        "# 2️⃣ Support Vector Machine\n",
        "svm = SVC().fit(X_train_vec, y_train)\n",
        "pred_svm = svm.predict(X_test_vec)\n",
        "print(\"\\nSVM:\\n\", metrics.classification_report(y_test, pred_svm))\n",
        "\n",
        "# 3️⃣ Random Forest\n",
        "rf = RandomForestClassifier().fit(X_train_vec, y_train)\n",
        "pred_rf = rf.predict(X_test_vec)\n",
        "print(\"\\nRandom Forest:\\n\", metrics.classification_report(y_test, pred_rf))"
      ],
      "metadata": {
        "id": "xJIxo-qeS48j",
        "outputId": "558dd956-a9ce-4282-cc19-12f0ef030c44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.89      0.77      0.83       667\n",
            "           P       0.80      0.91      0.85       677\n",
            "\n",
            "    accuracy                           0.84      1344\n",
            "   macro avg       0.85      0.84      0.84      1344\n",
            "weighted avg       0.85      0.84      0.84      1344\n",
            "\n",
            "\n",
            "SVM:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.93      0.68      0.79       667\n",
            "           P       0.75      0.95      0.84       677\n",
            "\n",
            "    accuracy                           0.82      1344\n",
            "   macro avg       0.84      0.81      0.81      1344\n",
            "weighted avg       0.84      0.82      0.81      1344\n",
            "\n",
            "\n",
            "Random Forest:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          IP       0.92      0.65      0.76       667\n",
            "           P       0.73      0.95      0.83       677\n",
            "\n",
            "    accuracy                           0.80      1344\n",
            "   macro avg       0.83      0.80      0.79      1344\n",
            "weighted avg       0.83      0.80      0.79      1344\n",
            "\n"
          ]
        }
      ]
    }
  ]
}